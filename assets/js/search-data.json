{
  
    
        "post0": {
            "title": "Self-supervised learning and soft-labeling for Image Classification",
            "content": "In this blog, we will learn/apply self-supervised learning and soft-label/pseudo-labelling technique to tackle the plant pathology Kaggle competition. I was introduced to soft-labeling when I came across this amazing blog by Isaac Flath. About the same time, I came across Jeremy&#39;s blog on self-supervised learning (SSL). Both the techniques were interesting and I feel going forward these techniques are going to be important techniques in computer vision. Hence, I decided to blog my learnings of these intriguing techniques. . A word on the dataset . For this blog, we will be using the Plant Pathology dataset. The task at hand is to classify images of different plant diseases. . There are some challenges with the dataset . Imbalanced data - one of the class contains very few samples | Mislabeled data - the other issue with this dataset is that the labels are noisy/mislabeled. The winner of this competition used soft-labelling in the winning model | We will do this in two parts . Self-supervised learning . What is self-supervised learning | Different types of self-supervised learning | Applying rotation-based learning (rotnet) | Applying SimCLR learning | | Soft-labeling | pseudo-laebling . Generating pseudo-label using transfer learning from rotnet model, SimCLR model | Final task (downstream task) to predict plant diseases using transfer learning (using the model trained in part 1) and soft-labeling/progressive pseudo-labelling | | Part 1: Self-Supervised Learning . What is Self-supervised learning? . Supervised learning has brought tremendous success to the field of computer vision. Although supervised learning requires huge amount of data, by using transfer learning one could reduce the requirement of data by about 1000x. . Largely, models for transfer learning are trained using the ImageNet dataset which were trained on 1.4 million images of 1000 different classes. Sometimes, for some domain, such as medical images, transfer learning from ImageNet might not work that well. Given this constraint, and the fact that generating labels for large amount of data in this domain is costly, self-supervised learning (SSL) has proven to be effective. In SSL, unlabeled data is trained in a supervised manner. . So what is self-supervised learning? SSL make use of the labels that are naturally part of the input data. SSL has been widely used in NLP tasks. Training a language model often involves predicting the next word of a sentence. The label for the language model, the next word, is a natural part of the input data. While learning to predict the next word, the model must have learnt a bit about the nature of language. Now, such training are common in most NLP tasks. . In computer vision, SSL starts with &quot;pretext tasks&quot;. In &quot;pretext task&quot;, we train a model from scratch using labels that comes naturally with the input data. Once pretrained, fine-tuning can be carried out on the &quot;downstream tasks&quot;. . Further Readings . https://www.fast.ai/2020/01/13/self_supervised/ . https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html . Different ways of doing self-supervised learning . Different techniques of SSL can be categorized into three . Pretext task based . Relative positioning | Colorization | Rotation | Multiple pretext | Jigsaw puzzle | | Generative model based . Autoencoders | Split brain autoencoder | Neural scene representation | Context encoder | Semantic inpainting | BiGAN | | Discriminative based contrastive learning . SimCLR | SimCLR2 | MoCo | MoCo v2 | BYOL | SwAV | | SSL and different techniques were well explained in this lecture series by Anuj Shah. Taking a look a the SOTA page for SSL on PapersWithCode. The discriminative based contrastive learning are leading the pack. . Rotation-based pretext tasks . In this pretext task, we rotate the image by certain degrees (0, 90, 180 and 270) and train the model to predict which degree of rotation was applied. While learning to classify the degree of rotation, the model learns many semantic concepts. . (Image source: Gidaris et al. 2018) https://arxiv.org/abs/1803.07728 . Now, let&#39;s take a look how we can apply this pretext task using fastai. For a more detailed explanation, please refer to this amazing blog by Amar Saini. . from fastai.vision.all import * from fastai.vision.learner import _update_first_layer import timm import torchvision . . path = Path(&#39;/content/drive/MyDrive/colab_notebooks/fastai/plant_pathology/data&#39;) path_img = path/&#39;images&#39; train = pd.read_csv(path/&#39;train.csv&#39;) train.head(5) . . We will use seresnext50_32x4d from timm model as the architecture of choice. . def create_timm_body(arch:str, pretrained=False, cut=None, n_in=3): &quot;Creates a body from any model in the `timm` library.&quot; model = timm.create_model(arch, pretrained=pretrained, num_classes=0, global_pool=&#39;&#39;) _update_first_layer(model, n_in, pretrained) if cut is None: ll = list(enumerate(model.children())) cut = next(i for i,o in reversed(ll) if has_pool_type(o)) if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut]) elif callable(cut): return cut(model) else: raise NamedError(&quot;cut must be either integer or function&quot;) . arch = create_timm_body(&#39;seresnext50_32x4d&#39;) nf = num_features_model(arch) head = create_head(nf, 4, concat_pool=True) net = nn.Sequential(arch, head) . tensorToImage = torchvision.transforms.ToPILImage() imageToTensor = torchvision.transforms.ToTensor() . Once we have selected the encoder/base architecture, it&#39;s time to build our PyTorch style dataset. There is nothing fancy going on here except each training example is rotated randomly by 0, 90, 180 or 270 degrees. The degree of rotation will also be used as the label. . class Custom_Dataset_PP(): # Codes from Amar Saini&#39;s (__Epoching__) blog def __init__(self, fns): self.fns = fns def __len__(self): return len(self.fns) def __getitem__(self, idx): # 4 classes for rotation degrees = [0, 90, 180, 270] rand_choice = random.randint(0, len(degrees)-1) img = PILImage.create(self.fns[idx]) img = img.resize((256, 256)) img = img.rotate(degrees[rand_choice]) img = imageToTensor(img) return img, torch.tensor(rand_choice).long() def show_batch(self, n=3): fig, axs = plt.subplots(n, n, figsize=(12,10)) fig.tight_layout() for i in range(n): for j in range(n): rand_idx = random.randint(0, len(self)-1) img, label = self.__getitem__(rand_idx) axs[i, j].imshow(tensorToImage(img), cmap=&#39;gray&#39;) axs[i, j].set_title(&#39;Label: {0} ({1} Degrees)&#39;.format(label.item(), label.item()*90)) axs[i, j].axis(&#39;off&#39;) ds = Custom_Dataset_PP(path_img.ls()) . . As we can see from the show_batch, the rotation based pretext task might not be good for this dataset as leaves can exist in any orientations. But our objective is to learn hence this is still a good practice. . ds.show_batch() . From here, we use standard fastai practice to train the model. . split = int(len(path_img.ls())*0.8) train_fns = path_img.ls()[:split] valid_fns = path_img.ls()[split:] train_ds = Custom_Dataset_PP(train_fns) valid_ds = Custom_Dataset_PP(valid_fns) dls = DataLoaders.from_dsets(train_ds, valid_ds).cuda() . learn = Learner(dls, net, loss_func=CrossEntropyLossFlat(), splitter=default_split, metrics=accuracy) . learn.fit_one_cycle(25, 1e-4) . epoch train_loss valid_loss accuracy time . 0 | 1.569923 | 1.409840 | 0.271635 | 06:39 | . 1 | 1.523075 | 1.378337 | 0.320913 | 03:21 | . 2 | 1.461118 | 1.298200 | 0.352163 | 03:20 | . 3 | 1.364290 | 1.330265 | 0.362981 | 03:21 | . 4 | 1.270127 | 1.191683 | 0.418269 | 03:20 | . 5 | 1.204014 | 2.085269 | 0.301683 | 03:19 | . 6 | 1.129306 | 1.249018 | 0.411058 | 03:19 | . 7 | 1.054507 | 1.541547 | 0.355769 | 03:18 | . 8 | 0.996718 | 2.167537 | 0.304087 | 03:20 | . 9 | 0.930008 | 2.989300 | 0.269231 | 03:20 | . 10 | 0.888594 | 0.878343 | 0.522837 | 03:19 | . 11 | 0.852690 | 0.867087 | 0.533654 | 03:20 | . 12 | 0.810870 | 0.848070 | 0.554087 | 03:20 | . 13 | 0.786548 | 1.054648 | 0.491587 | 03:21 | . 14 | 0.739632 | 1.170150 | 0.473558 | 03:21 | . 15 | 0.707019 | 0.832262 | 0.542067 | 03:21 | . 16 | 0.671642 | 0.802649 | 0.585337 | 03:21 | . 17 | 0.653890 | 0.827771 | 0.569712 | 03:21 | . 18 | 0.621239 | 1.211577 | 0.491587 | 03:20 | . 19 | 0.600797 | 0.804843 | 0.573317 | 03:22 | . 20 | 0.581002 | 0.796838 | 0.580529 | 03:21 | . 21 | 0.564937 | 0.779816 | 0.587740 | 03:20 | . 22 | 0.564841 | 0.760423 | 0.579327 | 03:20 | . 23 | 0.549947 | 0.773734 | 0.584135 | 03:20 | . 24 | 0.533107 | 0.759421 | 0.608173 | 03:20 | . We will save the weights of the encoader. . torch.save(learn.model[0], f&#39;{path}/seresnext50_32x4d_rotnetencoader.pth&#39;) . SimCLR: A Simple Framework for Contrastive Learning of Visual Representation . As we saw earlier in PapersWithCode SOTA page for SSL, discriminative based contrastive learning are among the most successful SSL techniques. SimCLR is one among these techniques that was introduced by Google Research. . Before we go further, let’s take a look at the definition of contrastive learning? . What is contrastive learning? It is a SSL technique to learn features of a dataset without labels by teaching the model which data points are similar and which are different. . The SimCLR framework proposes four key components for contrastive learning . Data Augmentation . Here, an image is randomly transformed into two correlated views. This forms a positive pair. SimCLR sequentially applies the following augmentations - random cropping, followed by resize back to the original size, random color distortions and random Gaussian blur. Two views coming from the same image would form a positive pair while two views coming from different image would form a negative pair. . | A neural network base encoder . A model backbone that extracts the feature maps. In our case, we will be using seresnext50_32x4d from timm . | Neural network projection head . This maps the feature maps into a vector space representation where the contrastive loss will be applied. Multilayer perceptron with single linear layer was used in the original paper. . | Contrastive loss . A loss function designed to evaluate how good a job the network is at picking up positive pairs and negative pairs. . | Let&#39;s apply SimCLR in fastai using this self-supervised library by Kerem Turgutlu. . from self_supervised.simclr import * . . First, lets make a general dataloader as per usual. . train[&#39;labels&#39;] = train.iloc[:, 1:].idxmax(axis=1) . fns = train.image_id.values . def label_fn(fn): return train[train[&#39;image_id&#39;] == fn][&#39;labels&#39;].values[0] label_fn(fns[0]) . &#39;scab&#39; . sz = 256 ds = Datasets(fns, [[lambda x: f&#39;{path_img}/{x}.jpg&#39;, PILImage.create], [label_fn, Categorize()]], splits=RandomSplitter()(fns)) dls = ds.dataloaders(bs=32, after_item=[ToTensor(), Resize(sz), IntToFloatTensor()]) . dls.show_batch() . Next, we will make the model which would include an encoader and a projection head. For this we will make use of self-supervised library with some changes. The encoader will come from timm and then attach the projection head to the encoader. . def create_timm_body(arch:str, pretrained=False, cut=None, n_in=3): &quot;Creates a body from any model in the `timm` library.&quot; model = timm.create_model(arch, pretrained=pretrained, num_classes=0, global_pool=&#39;&#39;) _update_first_layer(model, n_in, pretrained) if cut is None: ll = list(enumerate(model.children())) cut = next(i for i,o in reversed(ll) if has_pool_type(o)) if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut]) elif callable(cut): return cut(model) else: raise NamedError(&quot;cut must be either integer or function&quot;) . class MLP(Module): &quot;MLP module as described in paper&quot; def __init__(self, dim, concat_pool=True, projection_size=128, hidden_size=256): self.pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1) self.flatten = Flatten() self.net = nn.Sequential( nn.Linear(dim, hidden_size), nn.ReLU(inplace=True), nn.Linear(hidden_size, projection_size) ) def forward(self, x): x = self.pool(x) x = self.flatten(x) x = self.net(x) return x . def create_simclr_model(arch=resnet50, n_in=3, pretrained=False, cut=None, concat_pool=True, hidden_size=256, projection_size=128): &quot;Create SimCLR from a given arch&quot; encoder = create_timm_body(arch, pretrained, cut, n_in) with torch.no_grad(): representation = encoder(torch.randn((2, n_in, 128, 128))) projector = MLP(representation.size(1)*2, projection_size, hidden_size=hidden_size) apply_init(projector) return SimCLRModel(encoder, projector) . model = create_simclr_model(&#39;seresnext50_32x4d&#39;, pretrained=False) . Once we have the dataloader and the model, we will make a learner. The learner use a SimCLRLoss and SimCLR callback function. We will look at these codes to understand what is going on. . learn = Learner(dls, model, SimCLRLoss(temp=0.5), cbs=[SimCLR(size=128, color=False, stats=None)]) . Below we have the codes for the SimCLR callback. The callback upon initiation makes two augmentation function - aug1 and aug2. These will be used to make the two views of the same input image. As you can see from the get_aug_pipe function the augmentation uses RandomResizedCrop, RandomHorizontalFlip, ColorJitter, and RandomGrayscale. The callback before_batch makes the two images, concats them and use that as the inputs. The labels are also changed. Assuming the initial batch_size of 5, the labels would be [5, 6, 7, 8, 9, 0, 1, 2, 3, 4]. The first image is positive pair of 5th image, the second image is positive pair of 6th image, and so on. . def get_aug_pipe(size, stats=imagenet_stats, s=.6, color=True, xtra_tfms=[]): &quot;SimCLR augmentations&quot; tfms = [] tfms += [kornia.augmentation.RandomResizedCrop((size, size), scale=(0.2, 1.0), ratio=(3/4, 4/3))] tfms += [kornia.augmentation.RandomHorizontalFlip()] if color: tfms += [kornia.augmentation.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)] if color: tfms += [kornia.augmentation.RandomGrayscale(p=0.2)] tfms += xtra_tfms if stats is not None: tfms += [Normalize.from_stats(*stats)] pipe = Pipeline(tfms) pipe.split_idx = 0 return pipe . class SimCLR(Callback): &quot;SimCLR callback&quot; def __init__(self, size=256, **aug_kwargs): self.aug1 = get_aug_pipe(size, **aug_kwargs) self.aug2 = get_aug_pipe(size, **aug_kwargs) def before_batch(self): xi,xj = self.aug1(self.x), self.aug2(self.x) self.learn.xb = (torch.cat([xi, xj]),) bs = self.learn.xb[0].shape[0] self.learn.yb = (torch.arange(bs, device=self.dls.device).roll(bs//2),) def show_one(self): xb = TensorImage(self.learn.xb[0]) bs = len(xb)//2 i = np.random.choice(bs) xb = self.aug1.decode(xb.to(&#39;cpu&#39;).clone()).clamp(0,1) images = [xb[i], xb[bs+i]] show_images(images) . b = dls.one_batch() learn._split(b) learn(&#39;before_batch&#39;) . The modified image inputs; the initial batch of 32 is doubled. . learn.xb[0].shape . torch.Size([64, 3, 128, 128]) . Let&#39;s take a look at the modified labels. . learn.yb[0] . tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], device=&#39;cuda:0&#39;) . Now that we have taken a look at the callback, lets take a look at the loss function. . class SimCLRLoss(Module): &quot;SimCLR loss function&quot; def __init__(self, temp=0.1): self.temp = temp def forward(self, inp, targ): bs,feat = inp.shape csim = F.cosine_similarity(inp, inp.unsqueeze(dim=1), dim=-1)/self.temp csim = remove_diag(csim) targ = remove_diag(torch.eye(targ.shape[0], device=inp.device)[targ]).nonzero()[:,-1] return F.cross_entropy(csim, targ) . The loss function calculates the cosine_similarity of the vector representation from the projection head and then divides it by the hyperparameter temp. We remove the diagonals from the cosine similarity and the targets. Then we calculate the cross_entropy. . Let&#39;s fit_one_cycle. . learn.fit_one_cycle(25, 1e-4) . epoch train_loss valid_loss time . 0 | 4.004565 | 4.004914 | 01:20 | . 1 | 3.730691 | 3.483200 | 01:21 | . 2 | 3.505424 | 3.356370 | 01:21 | . 3 | 3.350685 | 3.181578 | 01:21 | . 4 | 3.256876 | 3.138497 | 01:21 | . 5 | 3.188638 | 3.078935 | 01:21 | . 6 | 3.124358 | 3.023555 | 01:22 | . 7 | 3.088448 | 3.021066 | 01:22 | . 8 | 3.056739 | 3.010895 | 01:21 | . 9 | 3.032721 | 3.060869 | 01:21 | . 10 | 3.009059 | 3.009850 | 01:22 | . 11 | 2.992094 | 2.977447 | 01:22 | . 12 | 2.992314 | 2.997800 | 01:21 | . 13 | 2.974692 | 3.005455 | 01:21 | . 14 | 2.962943 | 2.972467 | 01:22 | . 15 | 2.954012 | 2.910474 | 01:21 | . 16 | 2.947802 | 2.915481 | 01:21 | . 17 | 2.944147 | 2.911163 | 01:21 | . 18 | 2.933424 | 2.912643 | 01:21 | . 19 | 2.931176 | 2.908795 | 01:21 | . 20 | 2.928176 | 2.912873 | 01:21 | . 21 | 2.927793 | 2.922826 | 01:21 | . 22 | 2.928530 | 2.908896 | 01:21 | . 23 | 2.922103 | 2.900265 | 01:21 | . 24 | 2.918781 | 2.921983 | 01:22 | . Similar to out rotnet, we will save the weights of the encoader. . torch.save(learn.model.encoder, f&#39;{path}/seresnext50_32x4d_simclrencoader.pth&#39;) . Part 2: Softlabelling . Now that we have pre-trained our network using SSL techniques, we will move on to the downstream task. For our downstream task, we will also use soft-labeling to help our training. . Soft-labeling is very helpful in cases where the labels are noisy. The dataset we have been using, Plant Pathology, suffers from noisy labels. The winning solution for this competition used soft-labeling. I came across soft-labeling through Isaac&#39;s blog. For more comprehensive application of soft-labeling, please refer to Isaac&#39;s blog. . We will do this part in two steps . Step 1: Generating soft-labels . Step 2: Applying soft-labels in the downstream task . Generating softlabels . from sklearn.model_selection import StratifiedKFold . . So to apply soft-labeling, we will have to generate the pseudo-labels. We will use SSL trained models to generate our pseudo-labels. The following is the steps to make pseudo-labels. . Create kfold. In our case, we will use k=2 | Train the model using SSL trained weights and fit on the training data with the given labels | After sufficient training, we will generate prediction on the valid set. The prediction will be used as the pseudo-labels | Repeat step 2 and 3 for all different k valid sets | train.head(3) . image_id healthy multiple_diseases rust scab . 0 Train_0 | 0 | 0 | 0 | 1 | . 1 Train_1 | 0 | 1 | 0 | 0 | . 2 Train_2 | 1 | 0 | 0 | 0 | . train[&#39;labels&#39;] = train.iloc[:, 1:].idxmax(1) . N_FOLDS = 2 train[&#39;fold&#39;] = -1 strat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=42, shuffle=True) for i, (_, test_index) in enumerate(strat_kfold.split(train.image_id.values, train[&#39;labels&#39;].values)): train.iloc[test_index, -1] = i train[&#39;fold&#39;] = train[&#39;fold&#39;].astype(&#39;int&#39;) . def get_dls(df, size, fold, bs): batch_tfms = [Normalize.from_stats(*imagenet_stats)] dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=IndexSplitter(df.loc[df.fold==fold].index), getters=[ ColReader(&#39;image_id&#39;, pref=path/&#39;images&#39;, suff=&#39;.jpg&#39;), ColReader(&#39;labels&#39;) ], item_tfms=[RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)], batch_tfms=batch_tfms) return dblock.dataloaders(df, bs=bs) . def get_model(df, rotnet=True): if rotnet: arch = create_timm_body(&#39;seresnext50_32x4d&#39;) torch.load(f&#39;{path}/seresnext50_32x4d_rotnetencoader.pth&#39;) head = create_head(num_features_model(arch), df[&#39;labels&#39;].nunique()) apply_init(head) model = nn.Sequential(arch, head) else: arch = create_timm_body(&#39;seresnext50_32x4d&#39;) torch.load(f&#39;{path}/seresnext50_32x4d_simclrencoader.pth&#39;) head = create_head(num_features_model(arch), df[&#39;labels&#39;].nunique()) apply_init(head) model = nn.Sequential(arch, head) return model . splits, preds, targs, preds_c, = [],[],[],[] items = pd.DataFrame(columns = train.columns) for i in range(N_FOLDS): dls = get_dls(train, 228, i, bs=16) model = get_model(train, rotnet=False) learn = Learner(dls, model, metrics=[accuracy, RocAuc()]) learn.fine_tune(15, reset_opt=True) # store predictions p, t, c = learn.get_preds(ds_idx=1, with_decoded=True) preds.append(p); targs.append(t); preds_c.append(c); items = pd.concat([items, dls.valid.items]) . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 2.245435 | 2.073756 | 0.407245 | 0.591565 | 01:27 | . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.630546 | 1.246244 | 0.506037 | 0.681683 | 01:28 | . 1 | 1.536798 | 1.205274 | 0.535675 | 0.697780 | 01:27 | . 2 | 1.576910 | 1.541064 | 0.473106 | 0.684111 | 01:27 | . 3 | 1.486408 | 1.448349 | 0.553238 | 0.696692 | 01:27 | . 4 | 1.408332 | 1.150345 | 0.531284 | 0.731727 | 01:27 | . 5 | 1.292639 | 0.882109 | 0.663008 | 0.804221 | 01:26 | . 6 | 1.103595 | 1.071286 | 0.611416 | 0.795798 | 01:28 | . 7 | 0.991075 | 0.743046 | 0.710209 | 0.824435 | 01:28 | . 8 | 0.845321 | 0.623146 | 0.765093 | 0.851047 | 01:27 | . 9 | 0.790943 | 0.619872 | 0.750823 | 0.874319 | 01:31 | . 10 | 0.686515 | 0.568958 | 0.791438 | 0.891836 | 01:28 | . 11 | 0.655953 | 0.519997 | 0.818880 | 0.897981 | 01:30 | . 12 | 0.597022 | 0.477010 | 0.830955 | 0.910269 | 01:28 | . 13 | 0.531461 | 0.475933 | 0.829857 | 0.908447 | 01:27 | . 14 | 0.502115 | 0.476578 | 0.828760 | 0.906857 | 01:27 | . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 2.179506 | 2.657610 | 0.390110 | 0.607213 | 01:26 | . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.620119 | 1.272247 | 0.509890 | 0.683388 | 01:26 | . 1 | 1.551659 | 1.186581 | 0.525275 | 0.723610 | 01:25 | . 2 | 1.457448 | 1.184272 | 0.515385 | 0.723678 | 01:24 | . 3 | 1.449472 | 1.069529 | 0.583516 | 0.748897 | 01:27 | . 4 | 1.385499 | 1.160630 | 0.567033 | 0.734892 | 01:26 | . 5 | 1.177015 | 1.050339 | 0.606593 | 0.790950 | 01:25 | . 6 | 1.101527 | 0.718146 | 0.713187 | 0.833402 | 01:25 | . 7 | 1.001735 | 0.937197 | 0.668132 | 0.813787 | 01:25 | . 8 | 0.915719 | 0.711719 | 0.735165 | 0.857576 | 01:26 | . 9 | 0.798974 | 0.606332 | 0.772527 | 0.870674 | 01:26 | . 10 | 0.707872 | 0.535294 | 0.794505 | 0.914215 | 01:26 | . 11 | 0.652735 | 0.533704 | 0.804396 | 0.901796 | 01:25 | . 12 | 0.597695 | 0.484282 | 0.832967 | 0.915541 | 01:25 | . 13 | 0.559858 | 0.475080 | 0.826374 | 0.920336 | 01:25 | . 14 | 0.542785 | 0.470118 | 0.825275 | 0.920716 | 01:24 | . imgs = L(o for o in items.image_id.values) y_true = L(o for o in items.labels.values) y_targ = L(dls.vocab[o] for o in torch.cat(targs)) y_pred = L(dls.vocab[o] for o in torch.cat(preds_c)) p_max = torch.cat(preds).max(dim=1)[0] . res = pd.DataFrame({&#39;imgs&#39;:imgs,&#39;y_true&#39;:y_true,&#39;y_pred&#39;:y_pred}).set_index(&#39;imgs&#39;) print(res.shape) print(train.shape) res.sample(5) . (1821, 2) (1821, 7) . y_true y_pred . imgs . Train_1305 scab | scab | . Train_544 healthy | healthy | . Train_1286 healthy | healthy | . Train_814 rust | rust | . Train_1694 healthy | healthy | . res.to_csv(f&#39;{path}/train_simclr_sl.csv&#39;) . Final downstream tasks . For our final task, we will train using the SSL trained models and use the pseudolebels we generated to &#39;reduce&#39; the impact of nosiy labels. . # dataframe with softlabels rotnet_df = pd.read_csv(f&#39;{path}/train_rotnet_sl.csv&#39;) simclr_df = pd.read_csv(f&#39;{path}/train_simclr_sl.csv&#39;) . rotnet_df.columns = [&#39;image_id&#39;, &#39;labels&#39;, &#39;softlabels&#39;] simclr_df.columns = [&#39;image_id&#39;, &#39;labels&#39;, &#39;softlabels&#39;] . from sklearn.model_selection import StratifiedKFold import gc N_FOLDS = 3 rotnet_df[&#39;fold&#39;] = -1 strat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=42, shuffle=True) for i, (_, test_index) in enumerate(strat_kfold.split(rotnet_df.image_id.values, rotnet_df[&#39;labels&#39;].values)): rotnet_df.iloc[test_index, -1] = i rotnet_df[&#39;fold&#39;] = rotnet_df[&#39;fold&#39;].astype(&#39;int&#39;) . N_FOLDS = 3 simclr_df[&#39;fold&#39;] = -1 strat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=42, shuffle=True) for i, (_, test_index) in enumerate(strat_kfold.split(simclr_df.image_id.values, simclr_df[&#39;labels&#39;].values)): simclr_df.iloc[test_index, -1] = i simclr_df[&#39;fold&#39;] = simclr_df[&#39;fold&#39;].astype(&#39;int&#39;) . class SoftLabelCB(Callback): def __init__(self, df_preds, y_true_weight = 0.5): &#39;&#39;&#39;df_preds is a pandas dataframe where index is image paths Must have y_true and y_pred one hot encoded columns (ie y_true_0, y_true_1) &#39;&#39;&#39; self.y_true_weight = y_true_weight self.y_pred_weight = 1 - y_true_weight self.df = pd.get_dummies(df_preds, columns=[&#39;labels&#39;, &#39;softlabels&#39;]) def before_train(self): if type(self.dl.items)==type(pd.DataFrame()): self.idx_list = L(o for o in self.dl.items.index.values) if is_listy(self.dl.items): self.imgs_list = L(self.dl.items) def before_validate(self): if type(self.dl.items)==type(pd.DataFrame()): self.idx_list = L(o for o in self.dl.items.index.values) if is_listy(self.dl.items): self.imgs_list = L(self.dl.items) def before_batch(self): # get the images&#39; names for the current batch idx = self.idx_list[self.dl._DataLoader__idxs[self.iter*self.dl.bs:self.iter*self.dl.bs+self.dl.bs]] # get soft labels df = self.df soft_labels = df.loc[idx,df.columns.str.startswith(&#39;labels&#39;)].values if self.training: soft_labels = soft_labels*self.y_true_weight + df.loc[idx,df.columns.str.startswith(&#39;softlabels&#39;)].values*self.y_pred_weight self.learn.yb = (Tensor(soft_labels).cuda(),) class CrossEntropyLossOneHot(nn.Module): def __init__(self): super(CrossEntropyLossOneHot, self).__init__() self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, preds, labels): return torch.mean(torch.sum(-labels * self.log_softmax(preds), -1)) def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred,targ = flatten_check(inp.argmax(dim=axis), targ.argmax(dim=axis)) return (pred == targ).float().mean() . for i in range(N_FOLDS): gc.collect() dls = get_dls(rotnet_df, 228, i, bs=32) model = get_model(rotnet_df, True) sm = SaveModelCallback(monitor=&#39;valid_loss&#39;, fname=f&#39;{path}/model/rotnet_downstm_fold_{i}_best&#39;) learn = Learner(dls, model, loss_func=CrossEntropyLossOneHot(), metrics=[accuracy, RocAuc()], cbs=[SoftLabelCB(rotnet_df)]) learn.fine_tune(25, base_lr=1e-3, reset_opt=True, cbs=sm, freeze_epochs=2) . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 2.180979 | 1.798362 | 0.400330 | 0.617794 | 06:34 | . 1 | 1.896754 | 1.784741 | 0.444811 | 0.667562 | 01:35 | . Better model found at epoch 0 with valid_loss value: 1.798362374305725. Better model found at epoch 1 with valid_loss value: 1.7847411632537842. . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.484100 | 1.153402 | 0.556837 | 0.730138 | 01:35 | . 1 | 1.320654 | 1.144631 | 0.570017 | 0.770639 | 01:34 | . 2 | 1.314283 | 1.177314 | 0.561779 | 0.756559 | 01:35 | . 3 | 1.311820 | 1.222077 | 0.563427 | 0.743319 | 01:35 | . 4 | 1.308478 | 1.284149 | 0.558484 | 0.765968 | 01:35 | . 5 | 1.241711 | 1.096887 | 0.596376 | 0.773927 | 01:35 | . 6 | 1.205534 | 1.040225 | 0.611203 | 0.790580 | 01:33 | . 7 | 1.109477 | 1.162204 | 0.619440 | 0.773357 | 01:35 | . 8 | 1.038942 | 0.975802 | 0.647446 | 0.827652 | 01:35 | . 9 | 0.926236 | 0.790048 | 0.728171 | 0.870287 | 01:35 | . 10 | 0.850851 | 0.666998 | 0.757825 | 0.879508 | 01:35 | . 11 | 0.766376 | 0.633047 | 0.775947 | 0.898495 | 01:35 | . 12 | 0.690296 | 0.546291 | 0.785832 | 0.924340 | 01:35 | . 13 | 0.646431 | 0.558037 | 0.790774 | 0.916405 | 01:36 | . 14 | 0.617734 | 0.487432 | 0.823723 | 0.928350 | 01:35 | . 15 | 0.578832 | 0.504875 | 0.827018 | 0.922318 | 01:34 | . 16 | 0.555530 | 0.594202 | 0.812191 | 0.901316 | 01:35 | . 17 | 0.524157 | 0.443095 | 0.836903 | 0.941674 | 01:35 | . 18 | 0.490369 | 0.416304 | 0.851730 | 0.942821 | 01:35 | . 19 | 0.476614 | 0.419605 | 0.855025 | 0.944162 | 01:36 | . 20 | 0.453619 | 0.403086 | 0.850082 | 0.946465 | 01:35 | . 21 | 0.438267 | 0.398012 | 0.863262 | 0.945645 | 01:36 | . 22 | 0.421713 | 0.392551 | 0.858320 | 0.947091 | 01:36 | . 23 | 0.406884 | 0.391196 | 0.858320 | 0.947049 | 01:36 | . 24 | 0.416799 | 0.393447 | 0.855025 | 0.948277 | 01:34 | . Better model found at epoch 0 with valid_loss value: 1.1534024477005005. Better model found at epoch 1 with valid_loss value: 1.1446309089660645. Better model found at epoch 5 with valid_loss value: 1.096887469291687. Better model found at epoch 6 with valid_loss value: 1.0402253866195679. Better model found at epoch 8 with valid_loss value: 0.9758020043373108. Better model found at epoch 9 with valid_loss value: 0.7900478839874268. Better model found at epoch 10 with valid_loss value: 0.6669976115226746. Better model found at epoch 11 with valid_loss value: 0.6330470442771912. Better model found at epoch 12 with valid_loss value: 0.546291172504425. Better model found at epoch 14 with valid_loss value: 0.48743197321891785. Better model found at epoch 17 with valid_loss value: 0.4430951774120331. Better model found at epoch 18 with valid_loss value: 0.41630420088768005. Better model found at epoch 20 with valid_loss value: 0.40308573842048645. Better model found at epoch 21 with valid_loss value: 0.3980117738246918. Better model found at epoch 22 with valid_loss value: 0.392551451921463. Better model found at epoch 23 with valid_loss value: 0.39119648933410645. . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 2.077682 | 1.358871 | 0.421746 | 0.630542 | 01:35 | . 1 | 1.890515 | 1.920299 | 0.433278 | 0.624589 | 01:35 | . Better model found at epoch 0 with valid_loss value: 1.3588707447052002. . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.721737 | 1.420372 | 0.471170 | 0.660035 | 01:34 | . 1 | 1.556469 | 1.232172 | 0.560132 | 0.708895 | 01:35 | . 2 | 1.505628 | 1.309458 | 0.560132 | 0.715855 | 01:35 | . 3 | 1.521256 | 1.261295 | 0.537068 | 0.709441 | 01:35 | . 4 | 1.445887 | 1.404649 | 0.542010 | 0.675589 | 01:35 | . 5 | 1.421579 | 1.613186 | 0.505766 | 0.688969 | 01:35 | . 6 | 1.341298 | 1.260396 | 0.542010 | 0.698050 | 01:34 | . 7 | 1.266212 | 1.263892 | 0.599671 | 0.739613 | 01:35 | . 8 | 1.192686 | 1.052656 | 0.624382 | 0.782726 | 01:35 | . 9 | 1.123260 | 0.921180 | 0.640857 | 0.814421 | 01:35 | . 10 | 1.071517 | 0.980215 | 0.649094 | 0.796601 | 01:35 | . 11 | 1.003980 | 0.835629 | 0.698517 | 0.825559 | 01:35 | . 12 | 0.914666 | 1.014547 | 0.685338 | 0.805289 | 01:35 | . 13 | 0.815322 | 0.875604 | 0.696870 | 0.832545 | 01:35 | . 14 | 0.755339 | 0.692518 | 0.754530 | 0.861947 | 01:35 | . 15 | 0.682022 | 0.717943 | 0.764415 | 0.860634 | 01:35 | . 16 | 0.627511 | 0.689097 | 0.752883 | 0.853841 | 01:35 | . 17 | 0.552456 | 0.604317 | 0.797364 | 0.871979 | 01:36 | . 18 | 0.551212 | 0.627598 | 0.803954 | 0.880846 | 01:35 | . 19 | 0.544026 | 0.636285 | 0.803954 | 0.873143 | 01:35 | . 20 | 0.505864 | 0.585278 | 0.808896 | 0.886988 | 01:35 | . 21 | 0.483512 | 0.540007 | 0.828666 | 0.894359 | 01:35 | . 22 | 0.474091 | 0.579923 | 0.820428 | 0.883195 | 01:35 | . 23 | 0.456476 | 0.580129 | 0.825371 | 0.882503 | 01:35 | . 24 | 0.450166 | 0.562739 | 0.827018 | 0.885534 | 01:35 | . Better model found at epoch 0 with valid_loss value: 1.4203722476959229. Better model found at epoch 1 with valid_loss value: 1.232171893119812. Better model found at epoch 8 with valid_loss value: 1.052655577659607. Better model found at epoch 9 with valid_loss value: 0.9211796522140503. Better model found at epoch 11 with valid_loss value: 0.8356290459632874. Better model found at epoch 14 with valid_loss value: 0.6925175189971924. Better model found at epoch 16 with valid_loss value: 0.6890971064567566. Better model found at epoch 17 with valid_loss value: 0.6043166518211365. Better model found at epoch 20 with valid_loss value: 0.5852775573730469. Better model found at epoch 21 with valid_loss value: 0.5400067567825317. . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 2.081328 | 1.790862 | 0.342669 | 0.594329 | 01:33 | . 1 | 1.812289 | 1.978368 | 0.461285 | 0.698615 | 01:35 | . Better model found at epoch 0 with valid_loss value: 1.7908616065979004. . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.681780 | 1.351755 | 0.423394 | 0.678330 | 01:35 | . 1 | 1.628255 | 1.253813 | 0.533773 | 0.726907 | 01:35 | . 2 | 1.514754 | 1.179286 | 0.543657 | 0.744522 | 01:35 | . 3 | 1.446746 | 1.149792 | 0.537068 | 0.747809 | 01:35 | . 4 | 1.452345 | 1.244986 | 0.546952 | 0.685498 | 01:35 | . 5 | 1.401141 | 1.286189 | 0.560132 | 0.746649 | 01:35 | . 6 | 1.342116 | 1.166848 | 0.556837 | 0.741916 | 01:35 | . 7 | 1.276286 | 1.104091 | 0.593081 | 0.772318 | 01:33 | . 8 | 1.199893 | 1.050359 | 0.614498 | 0.784405 | 01:35 | . 9 | 1.111698 | 0.914566 | 0.640857 | 0.823923 | 01:35 | . 10 | 1.059557 | 0.892816 | 0.665568 | 0.839800 | 01:35 | . 11 | 0.988732 | 0.863787 | 0.688633 | 0.842417 | 01:35 | . 12 | 0.886488 | 0.689515 | 0.746293 | 0.871834 | 01:35 | . 13 | 0.803669 | 0.739485 | 0.744646 | 0.869706 | 01:35 | . 14 | 0.719890 | 0.716404 | 0.757825 | 0.902299 | 01:35 | . 15 | 0.649968 | 0.674304 | 0.775947 | 0.899190 | 01:34 | . 16 | 0.613759 | 0.650748 | 0.761120 | 0.902635 | 01:33 | . 17 | 0.573795 | 0.488263 | 0.835255 | 0.932428 | 01:35 | . 18 | 0.549245 | 0.499743 | 0.831960 | 0.928918 | 01:35 | . 19 | 0.518839 | 0.460697 | 0.835255 | 0.935363 | 01:35 | . 20 | 0.480734 | 0.425933 | 0.851730 | 0.945728 | 01:35 | . 21 | 0.465403 | 0.426006 | 0.855025 | 0.945700 | 01:35 | . 22 | 0.442722 | 0.419363 | 0.853377 | 0.947554 | 01:35 | . 23 | 0.466067 | 0.409462 | 0.861615 | 0.949312 | 01:35 | . 24 | 0.467920 | 0.411176 | 0.866557 | 0.949608 | 01:35 | . Better model found at epoch 0 with valid_loss value: 1.3517550230026245. Better model found at epoch 1 with valid_loss value: 1.2538130283355713. Better model found at epoch 2 with valid_loss value: 1.1792856454849243. Better model found at epoch 3 with valid_loss value: 1.1497923135757446. Better model found at epoch 7 with valid_loss value: 1.104090929031372. Better model found at epoch 8 with valid_loss value: 1.0503588914871216. Better model found at epoch 9 with valid_loss value: 0.9145664572715759. Better model found at epoch 10 with valid_loss value: 0.8928159475326538. Better model found at epoch 11 with valid_loss value: 0.8637869358062744. Better model found at epoch 12 with valid_loss value: 0.6895149350166321. Better model found at epoch 15 with valid_loss value: 0.6743040680885315. Better model found at epoch 16 with valid_loss value: 0.650747537612915. Better model found at epoch 17 with valid_loss value: 0.48826295137405396. Better model found at epoch 19 with valid_loss value: 0.4606971740722656. Better model found at epoch 20 with valid_loss value: 0.4259330928325653. Better model found at epoch 22 with valid_loss value: 0.419363409280777. Better model found at epoch 23 with valid_loss value: 0.4094623327255249. . Those are pretty amazing results! . References: . https://amarsaini.github.io/Epoching-Blog/jupyter/2020/03/23/Self-Supervision-with-FastAI.html . https://github.com/Isaac-Flath/fastblog/blob/master/_notebooks/2021-02-15-PlantPathology.ipynb . https://keremturgutlu.github.io/self_supervised/ . https://www.kaggle.com/keremt/progressive-label-correction-paper-implementation . https://paperswithcode.com/method/simclr . https://medium.com/wicds/exploring-the-essence-of-simclr-8e205ebc77af . https://www.pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/ .",
            "url": "https://moarshy.github.io/blogs/image%20classification/fastai/self-supervised/soft-label/2021/03/31/sslsoftlabeling.html",
            "relUrl": "/image%20classification/fastai/self-supervised/soft-label/2021/03/31/sslsoftlabeling.html",
            "date": " • Mar 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Continuing with DICOM",
            "content": "In this blog, we will largely use Jeremy Howard&#39;s series of notebooks on Kaggle to understand DICOM and to learn how to use fastai.medical to work with DICOM files. I would highly encourage going through Jeremy&#39;s blogs. Here, I will be merely be blogging as I learn and practice his notebooks. . There are five notebooks in this series. In this blog, we will go through the first four notebooks. . Creating a metadata DataFrame - In this notebook, we will learn how to create a DataFrame from DICOM metadata . | Some DICOM gotchas to be aware of - Here, we will learn some things to watch out for when working with DICOM like signed data being used as if it were unsigned . | Don&#39;t see like a radiologist - Radiologists, as we will learn below, use windowing to observe different area of interest such as brain, subdural because humans are limited in their ability to distinguish different contrast levels. This human limitations does not affect macines. In this notebook, we will learn how we can prepare (normalize) our images to help our model learn better. . | Cleaning the data for rapid prototyping - We will put our learnings into action by preparing a prototyping dataset. We will clean the data and then make a smaller set of smaller JPEG images for prototyping. . | From prototyping to submission - In this final notebook, we will learn to use our prototype dataset in building a classifier then learn to use full scale dataset to train our classifier. not covered in this blog . | Importing . from fastai.basics import * from fastai.vision.all import * from fastai.medical.imaging import * import seaborn as sns . Creating a metadata dataframe . The first notebook show us how to make a dataframe from dcm files. . path_dest = Path(&#39;./&#39;) . path = Path(&#39;../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection&#39;) . path_trn = path/&#39;stage_2_train&#39; fns_trn = path_trn.ls() . path_tst = path/&#39;stage_2_test&#39; fns_tst = path_tst.ls() . fn = fns_trn[0] dcm = fn.dcmread() dcm . Dataset.file_meta - (0002, 0002) Media Storage SOP Class UID UI: CT Image Storage (0002, 0003) Media Storage SOP Instance UID UI: 10000000300633 (0002, 0010) Transfer Syntax UID UI: Implicit VR Little Endian (0002, 0012) Implementation Class UID UI: 1.2.3.4 (0002, 0013) Implementation Version Name SH: &#39;RSNA Challenge 2019&#39; - (0008, 0018) SOP Instance UID UI: ID_27a354d42 (0008, 0060) Modality CS: &#39;CT&#39; (0010, 0020) Patient ID LO: &#39;ID_907f00d7&#39; (0020, 000d) Study Instance UID UI: ID_f69dbbd67a (0020, 000e) Series Instance UID UI: ID_8408ebcd9f (0020, 0010) Study ID SH: &#39;&#39; (0020, 0032) Image Position (Patient) DS: [-125, -18, 111.900024] (0020, 0037) Image Orientation (Patient) DS: [1, 0, 0, 0, 1, 0] (0028, 0002) Samples per Pixel US: 1 (0028, 0004) Photometric Interpretation CS: &#39;MONOCHROME2&#39; (0028, 0010) Rows US: 512 (0028, 0011) Columns US: 512 (0028, 0030) Pixel Spacing DS: [0.48828125, 0.48828125] (0028, 0100) Bits Allocated US: 16 (0028, 0101) Bits Stored US: 12 (0028, 0102) High Bit US: 11 (0028, 0103) Pixel Representation US: 0 (0028, 1050) Window Center DS: [00036, 00036] (0028, 1051) Window Width DS: [00080, 00080] (0028, 1052) Rescale Intercept DS: &#34;-1024.0&#34; (0028, 1053) Rescale Slope DS: &#34;1.0&#34; (7fe0, 0010) Pixel Data OW: Array of 524288 elements . Looking at an example dcm file we can see that we have seen and cover most of the data elements in our previous blog except Window Center, Window Width, Rescale Intercept and Rescale Slope. We will focus on them. . (0028, 1050) Window Center and (0028, 1051) Window Width The grayscale values of a CT is made up of a range of pixel values. Since they are either 12-bit or 16-bit, the range of possible values are 0-4096 (for 12-bit) and 0-65536 (for 16-bit). This is in comparison to a normal 8-bit images whose values range between 0-255. The larger the range, larger is the levels of contrast. Our human eyes are merely good at contrasting 100 levels of contrast. A 12-bit image would therefore provide 4096 contrast levels which could be difficult for humans to differentiate. Hence, in CTs, it is normal to choose a range of value to observe. The midlevel of the range is the Window Center and the range is Window Level. We can also think of varying Window Center as varying the brighness and varying the Window Level as varying the contrast. . fastai provides the Window Level and Window Center for observing different area of interest. Let&#39;s take a look at few . dicom_windows . namespace(brain=(80, 40), subdural=(254, 100), stroke=(8, 32), brain_bone=(2800, 600), brain_soft=(375, 40), lungs=(1500, -600), mediastinum=(350, 50), abdomen_soft=(400, 50), liver=(150, 30), spine_soft=(250, 50), spine_bone=(1800, 400)) . dicom_windows.brain . (80, 40) . dicom_windows.subdural . (254, 100) . Let&#39;s use fastai functionality to see some of these images under different windowing. . Note: We will learn about normalized windowing later in the blog . scales = False, True, dicom_windows.brain, dicom_windows.subdural titles = &#39;raw&#39;,&#39;normalized&#39;,&#39;brain windowed&#39;,&#39;subdural windowed&#39; for s,a,t in zip(scales, subplots(2,2,imsize=5)[1].flat, titles): dcm.show(scale=s, ax=a, title=t) . Rescale Intercept and Rescale Slope Now, let&#39;s move to understanding Rescale Intercept and Rescale Slope. The concept is pretty simple. DICOM uses linear transformation to save pixel values when stored on disk and when it is moved to memory. . y = mx + c . m is the Rescale Slope and c is the Rescale Intercept. . Why is this linear transformation needed? CT scans are meaured in Hounsfield Units which can be negative and they are stored as unsigned integers format which goes from 0 and above. Hence, a linear transformation is needed to shift the range of values. . Now that we have understood some of the important data elements. Let&#39;s prepare the dataframes so it would be easier to work with. fastai/pydicom provides really easy way to convert DICOM to dataframe. . . Note: Preparing these dataframes from DICOM takes really long. When I ran it on Kaggle, it took about 8 hours to make df_trn and df_tst below. . def save_lbls(): path_lbls = path/&#39;stage_2_train.csv&#39; lbls = pd.read_csv(path_lbls) lbls[[&quot;ID&quot;,&quot;htype&quot;]] = lbls.ID.str.rsplit(&quot;_&quot;, n=1, expand=True) lbls.drop_duplicates([&#39;ID&#39;,&#39;htype&#39;], inplace=True) pvt = lbls.pivot(&#39;ID&#39;, &#39;htype&#39;, &#39;Label&#39;) pvt.reset_index(inplace=True) pvt.to_feather(&#39;labels.fth&#39;) . . #df_lbls.head(8) . . #df_tst.to_feather(&#39;df_tst.fth&#39;) #df_tst.head() . #df_trn.to_feather(&#39;df_trn.fth&#39;) . Some DICOM gotchas to be aware of . From here, we will work on Jeremy&#39;s &quot;Some DICOM gotchas to be aware of&quot;. . path_df = Path(&#39;../input/dataframes&#39;) path_df.ls() . (#3) [Path(&#39;../input/dataframes/df_tst.fth&#39;),Path(&#39;../input/dataframes/labels.fth&#39;),Path(&#39;../input/dataframes/df_trn.fth&#39;)] . df_lbls = pd.read_feather(path_df/&#39;labels.fth&#39;) df_tst = pd.read_feather(path_df/&#39;df_tst.fth&#39;) df_trn = pd.read_feather(path_df/&#39;df_trn.fth&#39;) . We merge the df_lbls dataframe with df_trn. . comb = df_trn.join(df_lbls.set_index(&#39;ID&#39;), &#39;SOPInstanceUID&#39;) assert not len(comb[comb[&#39;any&#39;].isna()]) . comb.head(10) . SOPInstanceUID Modality PatientID StudyInstanceUID SeriesInstanceUID StudyID ImagePositionPatient ImageOrientationPatient SamplesPerPixel PhotometricInterpretation ... img_max img_mean img_std img_pct_window any epidural intraparenchymal intraventricular subarachnoid subdural . 0 ID_27a354d42 | CT | ID_907f00d7 | ID_f69dbbd67a | ID_8408ebcd9f | | -125.000000 | 1.0 | 1 | MONOCHROME2 | ... | 2671 | 454.914642 | 616.514427 | 0.149364 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 ID_9ef779a18 | CT | ID_8bca9b69 | ID_ed4776c4b6 | ID_073c18243c | | -125.000000 | 1.0 | 1 | MONOCHROME2 | ... | 2778 | -330.188461 | 1415.697408 | 0.266693 | 1 | 0 | 1 | 0 | 1 | 1 | . 2 ID_5bed38bf6 | CT | ID_b501522f | ID_906af9ca62 | ID_719a4e6a78 | | -125.000000 | 1.0 | 1 | MONOCHROME2 | ... | 2687 | -128.333286 | 678.065725 | 0.004173 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 ID_286599272 | CT | ID_7ba59d14 | ID_9f39f65610 | ID_b55ce168f2 | | -137.500000 | 1.0 | 1 | MONOCHROME2 | ... | 2568 | 396.514767 | 528.022874 | 0.107368 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 ID_bba76cea8 | CT | ID_32c07778 | ID_f9641021e5 | ID_e04ed2ad5c | | -122.000000 | 1.0 | 1 | MONOCHROME2 | ... | 2413 | 521.192650 | 568.400446 | 0.263206 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 ID_a917377cd | CT | ID_e4488e09 | ID_f13c83c8ae | ID_91366e3a6e | | -107.800003 | 1.0 | 1 | MONOCHROME2 | ... | 2756 | -90.590172 | 1140.209042 | 0.115768 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 ID_7723b03a6 | CT | ID_a329da04 | ID_cd9053516e | ID_37b71d633c | | -125.000000 | 1.0 | 1 | MONOCHROME2 | ... | 2732 | -308.353733 | 949.515538 | 0.008362 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 ID_dbb1cc814 | CT | ID_afd8ebfe | ID_55af39ec1b | ID_eb25c44049 | | -107.000000 | 1.0 | 1 | MONOCHROME2 | ... | 2374 | 448.588409 | 488.864187 | 0.126221 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 ID_4b08fe185 | CT | ID_6338c4f1 | ID_a3b607ba3f | ID_468e9dedde | | -125.000000 | 1.0 | 1 | MONOCHROME2 | ... | 3082 | 66.582775 | 1203.552054 | 0.235271 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 ID_8192d735e | CT | ID_b4a5aa19 | ID_3f35938aea | ID_1ff3a48327 | | -132.000000 | 1.0 | 1 | MONOCHROME2 | ... | 2612 | 471.477554 | 578.011862 | 0.277931 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 rows × 48 columns . Looking at BitsStored and PixelRepresentation . Recap . BitsStored - Tells whether the data is stored in 12 or 16 bits . PixelRepresentation - Tells if the data it is signed or unsigned data. Signed data can have negative pixels while unsigned starts from 0 and above. . repr_flds = [&#39;BitsStored&#39;,&#39;PixelRepresentation&#39;] comb.pivot_table(values=[&#39;img_mean&#39;,&#39;img_max&#39;,&#39;img_min&#39;,&#39;PatientID&#39;,&#39;any&#39;], index=repr_flds, aggfunc={&#39;img_mean&#39;:&#39;mean&#39;,&#39;img_max&#39;:&#39;max&#39;,&#39;img_min&#39;:&#39;min&#39;,&#39;PatientID&#39;:&#39;count&#39;,&#39;any&#39;:&#39;mean&#39;}) . PatientID any img_max img_mean img_min . BitsStored PixelRepresentation . 12 0 333443 | 0.128409 | 4095 | 451.058719 | 0 | . 1 2312 | 0.335640 | 2047 | -639.495079 | -2048 | . 16 1 417048 | 0.154275 | 32767 | 41.121570 | -32768 | . We can see that that when PixelRepresentation is 1, meaning the data type is signed, img_min can take negative values. Largely, we will be working with unsigned 12-bit data and signed 16-bit data. . As we saw earlier RescaleIntercept and RescaleSlope tell us how to scale our data. Let&#39;s take a look. . comb.pivot_table(values=[&#39;WindowCenter&#39;,&#39;WindowWidth&#39;, &#39;RescaleIntercept&#39;, &#39;RescaleSlope&#39;], index=repr_flds, aggfunc={&#39;mean&#39;,&#39;max&#39;,&#39;min&#39;,&#39;std&#39;,&#39;median&#39;}) . RescaleIntercept RescaleSlope WindowCenter WindowWidth . max mean median min std max mean median min std max mean median min std max mean median min std . BitsStored PixelRepresentation . 12 0 1.0 | -1023.141245 | -1024.0 | -1024.0 | 19.947663 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 650.0 | 37.940949 | 36.0 | 25.0 | 19.130654 | 4095.0 | 85.132092 | 80.0 | 26.0 | 127.766167 | . 1 0.0 | 0.000000 | 0.0 | 0.0 | 0.000000 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 350.0 | 48.775952 | 40.0 | 40.0 | 47.549496 | 4000.0 | 179.653979 | 80.0 | 80.0 | 603.265211 | . 16 1 0.0 | -1016.754254 | -1024.0 | -1024.0 | 85.832168 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 800.0 | 33.426668 | 30.0 | 25.0 | 19.091547 | 3000.0 | 100.568292 | 80.0 | 60.0 | 95.569248 | . Firstly, we know that CT scans are measured in Hounsfield Units (HU) which takes both negative and positive values. HU measures the radiodensity (how much radiation is absorbed) by differnt materials such as air, fats, bones. Some radiodensity values of different materials are Air -100 HU; Fat -120 to -90 HU; Bone (Cortical) +500 to +1900 . Based on this, we expect the mean RescaleIntercept for PixelRepresentation 0 to be around -1,024 but in our case there seem to be some with RescaleIntercept not equlas to -1,024. . The issue here could be that some of the images were signed data but were treated as unsigned. Later we will look at how to deal with them. . Now, lets take a look scaled_px function that fastai provides to easliy scale pixels based on their RescaleIntercept and RescaleSlope . dcm = path_trn.ls(1)[0].dcmread() . dcm.pixels . tensor([[24., 22., 22., ..., 21., 17., 17.], [25., 25., 23., ..., 21., 20., 18.], [26., 27., 25., ..., 21., 20., 21.], ..., [17., 18., 19., ..., 19., 19., 20.], [23., 22., 21., ..., 20., 21., 21.], [22., 21., 20., ..., 20., 20., 20.]]) . plt.hist(dcm.pixels.flatten().numpy()); . dcm.scaled_px . tensor([[-1000., -1002., -1002., ..., -1003., -1007., -1007.], [ -999., -999., -1001., ..., -1003., -1004., -1006.], [ -998., -997., -999., ..., -1003., -1004., -1003.], ..., [-1007., -1006., -1005., ..., -1005., -1005., -1004.], [-1001., -1002., -1003., ..., -1004., -1003., -1003.], [-1002., -1003., -1004., ..., -1004., -1004., -1004.]]) . plt.hist(dcm.scaled_px.flatten().numpy()); . DON&#39;T see like a radiologist! . Next, we will work on Jeremy&#39;s &quot;DON&#39;T see like a radiologist!&quot; notebook. . In this notebook, Jeremy&#39;s presents an idea as to how to help our model see better. We have seen earlier windowing is used to help radiologists to vary contrast and brightness to observe difffernt areas of interests such as the brain and the subdural. The reason for windowing is humans are only able to contrast about 100 levels of contrast gradient but a 16-bit CT has 2^16 (65,536) levels of contrast gradient. This is beyond a human&#39;s ability to distinguish hence windowning is used but this is not a problem for computer. . Let&#39;s see an image without windowing vs brain_window. . fname = path_trn.ls(10)[0] _, axes = plt.subplots(1, 2, figsize=(10,8)) dcm = fname.dcmread() for ax, name, window in zip(axes.flat, [&#39;no windowing&#39;, &#39;brain wondow&#39;], [None, dicom_windows.brain]): dcm.show(scale=window, ctx=ax, title=name) . This limitations of number of contrast gradient is not at all an issue for computer but we do have to rescale to help our models. Let&#39;s take a look at Jeremy&#39;s proposal. Let&#39;s see the rescaling. . px = dcm.scaled_px.flatten() sns.histplot(px) . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . We see a highly bimodal distribution. Background pixels are around -1000, and the brain tissue pixels are around 0. The proposal is to use non-linear mapping designed to give us an equal number of pixels in each range. Let&#39;s see how that is done. . bins = px.freqhist_bins(20) sns.histplot(dcm.scaled_px.flatten(), bins=bins) . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . &#39;fastai.medical.imaging&#39; can apply that non-linear mapping for you. In fact, this is the default way of displaying a DICOM in fastai. This is the normalized windowing we saw earlier. . plt.imshow(dcm.hist_scaled(), cmap=plt.cm.bone); . dcm.show() . Creating a normalised dataset . Now that we know how to scale our dcm, let&#39;s take a look at preparing our dataset. Because the non-linear mapping we used varies from image to image, we will need to create a mapping that is appropriate for a wide range of images. . We will create a mapping for three different groups of images we saw earlier. . df1 = comb.query(&#39;(BitsStored==12) &amp; (PixelRepresentation==0)&#39;) df2 = comb.query(&#39;(BitsStored==12) &amp; (PixelRepresentation==1)&#39;) df3 = comb.query(&#39;BitsStored==16&#39;) dfs = L(df1,df2,df3) . To create the bins . we will grab a random image with each label (htypes) and one with no labels for all the three groups above | then we will read the dcm | put them into a tensor | get the bins from this set of images | use the bins in hist_scaled_px to get the scaled float tensor, which we can pass to our model | htypes = &#39;any&#39;,&#39;epidural&#39;,&#39;intraparenchymal&#39;,&#39;intraventricular&#39;,&#39;subarachnoid&#39;,&#39;subdural&#39; def get_samples(df): recs = [df.query(f&#39;{c}==1&#39;).sample() for c in htypes] recs.append(df.query(&#39;any==0&#39;).sample()) return pd.concat(recs).fname.values sample_fns = concat(*dfs.map(get_samples)) . sample_dcms = L(Path(o).dcmread() for o in sample_fns) samples = torch.stack(tuple(sample_dcms.attrgot(&#39;scaled_px&#39;))) samples.shape . torch.Size([21, 512, 512]) . bins = samples.freqhist_bins() plt.plot(bins, torch.linspace(0,1,len(bins))); . dcm.show(bins) . dcm.hist_scaled(bins), dcm.hist_scaled(bins).shape . (tensor([[0.1558, 0.1299, 0.1299, ..., 0.1169, 0.0649, 0.0649], [0.1688, 0.1688, 0.1429, ..., 0.1169, 0.1039, 0.0779], [0.1818, 0.1948, 0.1688, ..., 0.1169, 0.1039, 0.1169], ..., [0.0649, 0.0779, 0.0909, ..., 0.0909, 0.0909, 0.1039], [0.1429, 0.1299, 0.1169, ..., 0.1039, 0.1169, 0.1169], [0.1299, 0.1169, 0.1039, ..., 0.1039, 0.1039, 0.1039]]), torch.Size([512, 512])) . Since the non-linear mapping results in a almost uniform distribution between 0 and 1, we wont have a mean and std of 0 and 1. . scaled_samples = torch.stack(tuple(o.hist_scaled(bins) for o in sample_dcms)) scaled_samples.mean(),scaled_samples.std() . (tensor(0.4080), tensor(0.2989)) . Cleaning the data for rapid prototyping . In this notebook, we learn . how to fix images with incorrect RescaleIntercept | removing images with minimal useful information | make a smaller dataset that we can use for prototyping | crop images to just contain the brain area | carry out histogram rescaling and save it as JPEG | The idea, apart from fixing incorrect images, is to create a dataset for rapid prototyping. . Fixing incorrect RescaleIntercept . The problematic images were found in our df1. . df1 = comb.query(&#39;(BitsStored==12) &amp; (PixelRepresentation==0)&#39;) . def df2dcm(df): return L(Path(o).dcmread() for o in df.fname.values) . df_iffy = df1[df1.RescaleIntercept&gt;-100] dcms = df2dcm(df_iffy) _,axs = subplots(2,4, imsize=3) for i,ax in enumerate(axs.flat): dcms[i].show(ax=ax) . That does not look good at all. . dcm = dcms[2] d = dcm.pixel_array plt.hist(d.flatten()); . As explained previously, the mode for unsigned data appears around 0 but in our case the mode appears around 3000. This could be because signed data could have been treated as if it were unsigned data. As explained by Jeremy . My guess is that what happened in the &quot;iffy&quot; images is that they were actually signed data, &gt;but were treated as unsigned. If that&#39;s the case, the a value of -1000 or -1024 (the usual &gt;values for background pixels in signed data images) will have wrapped around to 4096-&gt;1000=3096. So we&#39;ll need to shift everything up by 1000, then move the values larger than &gt;2048 back to where they should have been. . The fix is . add all pixel values by +1000 | for values more than 4096, -1000 | set RescaleIntercept to -1000 | d += 1000 px_mode = scipy.stats.mode(d.flatten()).mode[0] d[d&gt;=px_mode] = d[d&gt;=px_mode] - px_mode dcm.PixelData = d.tobytes() dcm.RescaleIntercept = -1000 . plt.hist(dcm.pixel_array.flatten()); . _,axs = subplots(1,2) dcm.show(ax=axs[0]); dcm.show(dicom_windows.brain, ax=axs[1]) . def fix_pxrepr(dcm): if dcm.PixelRepresentation != 0 or dcm.RescaleIntercept&lt;-100: return x = dcm.pixel_array + 1000 px_mode = 4096 x[x&gt;=px_mode] = x[x&gt;=px_mode] - px_mode dcm.PixelData = x.tobytes() dcm.RescaleIntercept = -1000 . dcms = df2dcm(df_iffy) dcms.map(fix_pxrepr) _,axs = subplots(2,4, imsize=3) for i,ax in enumerate(axs.flat): dcms[i].show(ax=ax) . Images look better following our repair work . Remove useless images . To remove &#39;not so useful&#39; images we will identify what % of pixel of an images is in the brain window (0, 80). This is already in the dataframe we created earlier under img_pct_window. . plt.hist(comb.img_pct_window,40); . comb = comb.assign(pct_cut = pd.cut(comb.img_pct_window, [0,0.02,0.05,0.1,0.2,0.3,1])) comb.pivot_table(values=&#39;any&#39;, index=&#39;pct_cut&#39;, aggfunc=[&#39;sum&#39;,&#39;count&#39;]).T . pct_cut (0.0, 0.02] (0.02, 0.05] (0.05, 0.1] (0.1, 0.2] (0.2, 0.3] (0.3, 1.0] . sum any 69 | 775 | 3690 | 23411 | 61741 | 18246 | . count any 79144 | 22217 | 50928 | 269727 | 255338 | 67315 | . We can see that images with little brain tissue (&lt;2% of pixels) have almost no labels. So we will remove them. . comb.drop(comb.query(&#39;img_pct_window&lt;0.02&#39;).index, inplace=True) . Resample to 2/3 split . df_lbl = comb.query(&#39;any==True&#39;) n_lbl = len(df_lbl) n_lbl . 107863 . df_nonlbl = comb.query(&#39;any==False&#39;).sample(n_lbl//2) len(df_nonlbl) . 53931 . comb = pd.concat([df_lbl,df_nonlbl]) len(comb) . 161794 . Crop to just brain area&#182; . dcm = Path(dcms[10].filename).dcmread() fix_pxrepr(dcm) . px = dcm.windowed(*dicom_windows.brain) show_image(px); . blurred = gauss_blur2d(px, 100) show_image(blurred); . show_image(blurred&gt;0.3); . _,axs = subplots(1,4, imsize=3) for i,ax in enumerate(axs.flat): dcms[i].show(dicom_windows.brain, ax=ax) show_image(dcms[i].mask_from_blur(dicom_windows.brain), cmap=plt.cm.Reds, alpha=0.6, ax=ax) . def pad_square(x): r,c = x.shape d = (c-r)/2 pl,pr,pt,pb = 0,0,0,0 if d&gt;0: pt,pd = int(math.floor( d)),int(math.ceil( d)) else: pl,pr = int(math.floor(-d)),int(math.ceil(-d)) return np.pad(x, ((pt,pb),(pl,pr)), &#39;minimum&#39;) def crop_mask(x): mask = x.mask_from_blur(dicom_windows.brain) bb = mask2bbox(mask) if bb is None: return lo,hi = bb cropped = x.pixel_array[lo[0]:hi[0],lo[1]:hi[1]] return pad_square(cropped) . _,axs = subplots(1,2) dcm.show(ax=axs[0]) dcm_m = PILCTScan.create(crop_mask(dcm)) dcm_m.show(ax=axs[1]); . Save JPEG images . Now, we will learn to save our smaller images as JPEG for fast prototyping. First, we will sample our dataset to get out freqhist_bins. And then use the samples to calculate our bins. . htypes = &#39;any&#39;,&#39;epidural&#39;,&#39;intraparenchymal&#39;,&#39;intraventricular&#39;,&#39;subarachnoid&#39;,&#39;subdural&#39; def get_samples(df): recs = [df.query(f&#39;{c}==1&#39;).sample() for c in htypes] recs.append(df.query(&#39;any==0&#39;).sample()) return pd.concat(recs).fname.values sample_fns = concat(*dfs.map(get_samples)) sample_dcms = tuple(Path(o).dcmread().scaled_px for o in sample_fns) samples = torch.stack(sample_dcms) bins = samples.freqhist_bins() . sample_fns.shape . (21,) . bins, bins.shape . (tensor([-3024., -2048., -1014., -1007., -1005., -1004., -1003., -1002., -1001., -1000., -999., -998., -997., -995., -994., -993., -991., -990., -988., -985., -981., -977., -973., -970., -967., -964., -960., -956., -951., -945., -936., -924., -909., -891., -866., -835., -801., -758., -691., -574., -351., -126., -70., -41., -15., 4., 11., 17., 21., 23., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 40., 43., 48., 59., 81., 164., 284., 397., 607., 788., 958., 1152., 1390., 1656.]), torch.Size([78])) . with open(f&quot;{path_dest}/bin.pkl&quot;, &quot;wb&quot;) as output_file: pickle.dump(bins, output_file) . Next, we will make a function to read a single dcm file and then fixes them using the fix_pxrepr funtion we create earlier. . def dcm_tfm(fn): fn = Path(fn) try: x = fn.dcmread() fix_pxrepr(x) except Exception as e: print(fn,e) raise SkipItemException if x.Rows != 512 or x.Columns != 512: x.zoom_to((512,512)) return x.scaled_px . We will then make fastai&#39;s TfmDL to use parallel processing to process the images. . comb.fname.values . array([&#39;../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/ID_9ef779a18.dcm&#39;, &#39;../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/ID_ba857d33f.dcm&#39;, &#39;../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/ID_9dcc748d1.dcm&#39;, ..., &#39;../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/ID_8e9f72f61.dcm&#39;, &#39;../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/ID_240b4c94d.dcm&#39;, &#39;../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/ID_515ceb460.dcm&#39;], dtype=object) . fns = list(comb.fname.values) dest = path_dest/&#39;train_jpg&#39; dest.mkdir(exist_ok=True) # NB: Use bs=512 or 1024 when running on GPU bs=4 ds = Datasets(fns, [[dcm_tfm],[os.path.basename]]) dl = TfmdDL(ds, bs=bs, num_workers=2) . As we can see below, the dataloader will return a tuple of scaled_px and its corresponding filename. . dl.one_batch() . (tensor([[[-3024., -3024., -3024., ..., -3024., -3024., -3024.], [-3024., -3024., -3024., ..., -3024., -3024., -3024.], [-3024., -3024., -3024., ..., -3024., -3024., -3024.], ..., [-3024., -3024., -3024., ..., -3024., -3024., -3024.], [-3024., -3024., -3024., ..., -3024., -3024., -3024.], [-3024., -3024., -3024., ..., -3024., -3024., -3024.]], [[-1024., -1024., -1024., ..., -1024., -1024., -1024.], [-1024., -1024., -1024., ..., -1024., -1024., -1024.], [-1005., -1003., -1000., ..., -1004., -1001., -999.], ..., [ -953., -953., -959., ..., -553., -620., -688.], [ -955., -959., -961., ..., -752., -796., -830.], [ -961., -962., -959., ..., -872., -890., -903.]], [[-2048., -2048., -2048., ..., -2048., -2048., -2048.], [-2048., -2048., -2048., ..., -2048., -2048., -2048.], [-2048., -2048., -2048., ..., -2048., -2048., -2048.], ..., [-2048., -2048., -2048., ..., -2048., -2048., -2048.], [-2048., -2048., -2048., ..., -2048., -2048., -2048.], [-2048., -2048., -2048., ..., -2048., -2048., -2048.]], [[ -881., -877., -877., ..., -890., -890., -890.], [ -885., -881., -878., ..., -891., -890., -887.], [ -885., -885., -883., ..., -890., -887., -883.], ..., [ -868., -872., -870., ..., -877., -875., -865.], [ -869., -872., -868., ..., -875., -879., -871.], [ -872., -871., -866., ..., -871., -879., -879.]]]), (&#39;ID_9ef779a18.dcm&#39;, &#39;ID_ba857d33f.dcm&#39;, &#39;ID_9dcc748d1.dcm&#39;, &#39;ID_9b225ec8a.dcm&#39;)) . The following functions return output filename and save the cropped .jpg files. . def dest_fname(fname): return dest/Path(fname).with_suffix(&#39;.jpg&#39;) def save_cropped_jpg(o, dest): fname,px = o px.save_jpg(dest_fname(fname), [dicom_windows.brain, dicom_windows.subdural], bins=bins) . In the next function, we make the following . move the pixels from dataloader to device | make our masks for cropping only the brain portion | then make the crop | use parallel funtioality to save the images. The save_jpg will save brain window, subdural wondow and normalised. Each as one chanel. | def process_batch(pxs, fnames, n_workers=4): pxs = to_device(pxs) masks = pxs.mask_from_blur(dicom_windows.brain) bbs = mask2bbox(masks) gs = crop_resize(pxs, bbs, 256).cpu().squeeze() parallel(save_cropped_jpg, zip(fnames, gs), n_workers=n_workers, progress=False, dest=dest) . %time process_batch(*dl.one_batch(), n_workers=3) . CPU times: user 63.6 ms, sys: 130 ms, total: 194 ms Wall time: 373 ms . Let&#39;s open and see one of the images. . fn = dest.ls()[0] im = Image.open(fn) fn . Path(&#39;train_jpg/ID_ba857d33f.jpg&#39;) . As can be seen below, each channel in the saved jpg corresponds to brain window, subdural window and normalized. . axs = subplots(1, 3)[1].flat for i, ax in zip(tensor(im).permute(2,0,1), axs): ax.imshow(i) ax.axis(&#39;off&#39;) . The above can be simply shown using fastai functionality. . show_images(tensor(im).permute(2,0,1), titles=[&#39;brain&#39;,&#39;subdural&#39;,&#39;normalized&#39;]) . #dest.mkdir(exist_ok=True) #for b in progress_bar(dl): process_batch(*b, n_workers=8) . Building a classifier . For this part, we will be following some part of Jeremy&#39;s 5th notebook. For the full version, please refer to Jeremy&#39;s notebook. . df_comb = comb.set_index(&#39;SOPInstanceUID&#39;) df_tst = pd.read_feather(path_df/&#39;df_tst.fth&#39;) . The next two functions are what we looked in the previous section. We will use the fix_pxrepr function to fix the PixelRepresentation issue we encountered. And make a 3-channel image using brain window, subdural window and normalised image using the freq_his bins we developed earlier. . def fix_pxrepr(dcm): if dcm.PixelRepresentation != 0 or dcm.RescaleIntercept&lt;-100: return x = dcm.pixel_array + 1000 px_mode = 4096 x[x&gt;=px_mode] = x[x&gt;=px_mode] - px_mode dcm.PixelData = x.tobytes() dcm.RescaleIntercept = -1000 . def dcm_tfm(fn): fn = (path_trn/fn).with_suffix(&#39;.dcm&#39;) try: x = fn.dcmread() fix_pxrepr(x) except Exception as e: print(fn,e) raise SkipItemException if x.Rows != 512 or x.Columns != 512: x.zoom_to((512,512)) px = x.scaled_px return TensorImage(px.to_3chan(dicom_windows.brain,dicom_windows.subdural, bins=bins)) . Let&#39;s make a split based on PatientID . set_seed(42) patients = df_comb.PatientID.unique() pat_mask = np.random.random(len(patients))&lt;0.8 pat_trn = patients[pat_mask] . def split_data(df): idx = L.range(df) mask = df.PatientID.isin(pat_trn) return idx[mask],idx[~mask] splits = split_data(df_comb) . filename function returns the filename while fn2image opens the filename and returns PILDicom object . def filename(o): return os.path.splitext(os.path.basename(o))[0] fns = L(list(df_comb.fname)).map(filename) fn = fns[0] fn . &#39;ID_9ef779a18&#39; . def fn2image(fn): return PILDicom.create((path_trn/fn).with_suffix(&#39;.dcm&#39;)) fn2image(fn).show(); . fnlabel return the labels given a filename . htypes = [&#39;any&#39;,&#39;epidural&#39;,&#39;intraparenchymal&#39;,&#39;intraventricular&#39;,&#39;subarachnoid&#39;,&#39;subdural&#39;] def fn2label(fn): return df_comb.loc[fn][htypes].values.astype(np.float32) fn2label(fn) . array([1., 0., 1., 0., 1., 1.], dtype=float32) . get_loss prepares the weighted loss fuction . def get_loss(scale=1.0): loss_weights = tensor(2.0, 1, 1, 1, 1, 1).cuda()*scale return BaseLoss(nn.BCEWithLogitsLoss, pos_weight=loss_weights, floatify=True, flatten=False, is_2d=False) . accuracy_any calculates the accuracy for any label . def accuracy_any(inp, targ, thresh=0.5, sigmoid=True): inp,targ = flatten_check(inp[:,0],targ[:,0]) if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . loss_func = get_loss(0.14*2) opt_func = partial(Adam, wd=0.01, eps=1e-3) metrics=[accuracy_multi,accuracy_any] . Next, we will need to prepare the dataloaders. . First, we will need the transformations necessary to open/prepare (we will us dcm_tfm) the images and prepare the labels (we will use fn2label followed by EncodedMultiCategorize which makes one-hot encoded multi-category labels). . tfms = [[dcm_tfm], [fn2label, EncodedMultiCategorize(htypes)]] dls = Datasets(fns, tfms, splits=splits) nrm = Normalize(tensor([0.6]),tensor([0.25])) aug = aug_transforms(p_lighting=0.) batch_tfms = [nrm, *aug] . def get_data(bs, sz): return dls.dataloaders(bs=bs, num_workers=nw, after_item=[ToTensor], after_batch=batch_tfms+[AffineCoordTfm(size=sz)]) . dbch = get_data(64,256) x,y = dbch.one_batch() dbch.show_batch(max_n=4) x.shape . (64, 3, 256, 256) . bs = 32 sz=128 . def get_learner(bs, sz): dls = get_data(bs,sz) learn = cnn_learner(dls, xresnet50, loss_func=loss_func, opt_func=opt_func, metrics=metrics) return learn.to_fp16() . learn = get_learner(bs, sz) . learn.fit_one_cycle(1, 1e-3) . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss accuracy_multi accuracy_any time . . 89.97% [3649/4056 42:00&lt;04:41 0.1933] &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; This concludes the amazing series of notebooks by Jeremy! . References . https://www.kaggle.com/jhoward/creating-a-metadata-dataframe-fastai . https://www.kaggle.com/jhoward/some-dicom-gotchas-to-be-aware-of-fastai . https://www.kaggle.com/jhoward/don-t-see-like-a-radiologist-fastai . https://blog.kitware.com/dicom-rescale-intercept-rescale-slope-and-itk/#:~:text=What%20is%20Rescale%20Intercept%20%2F%20Rescale,to%20their%20in%20memory%20representation. . &lt;/div&gt; .",
            "url": "https://moarshy.github.io/blogs/dicom/medical%20images/fastai/2021/03/06/dicomcontinued.html",
            "relUrl": "/dicom/medical%20images/fastai/2021/03/06/dicomcontinued.html",
            "date": " • Mar 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "What is DICOM?",
            "content": ". Note: Most of the codes used in this blog are either taken from fastbook or fastai docs . What is DICOM? . DICOM stands for Digital Imaging and COmmunication in Medicine | It is a software integration standard that is used in medical imaging | It is the standard that establishes rules that allows different medical imaging modalities (such as X-Rays, Ultrasound, CT, MRI) from different vendors and hospitals to exchange information between them | . There core of DICOM . DICOM File Format | This is the important for part for DL. | DICOM images uses the .dcm extension | .dcm allows patient data, image pixel values to be stored under different tags | Like mentioned above apart from images, DICOM also contains patient details (such as patient name and age) and image acqusition data (such as type of equipment used) | . DICOM Network protocol | The protocol allows for information exchange between different imaging modalities that is connected to the hospital network | Used for searching for images from the archive and to display images on the workstation | This protocol can also be used to monitor treatment, schedule procedures, report status | . Let&#39;s use fastai to read a dcm file and use it to understand the information it contains. We will use the SIIM-ACR Pneumothorax Segentation dataset. . ! pip install fastai -q --upgrade ! pip install pydicom kornia opencv-python scikit-image nbdev -q . |████████████████████████████████| 194kB 18.8MB/s |████████████████████████████████| 61kB 10.7MB/s |████████████████████████████████| 1.9MB 18.6MB/s |████████████████████████████████| 225kB 46.4MB/s |████████████████████████████████| 51kB 8.3MB/s . import fastai print(fastai.__version__) . 2.2.5 . from fastai.basics import * from fastai.callback.all import * from fastai.vision.all import * from fastai.medical.imaging import * import pydicom import pandas as pd . # downloading the dataset pneumothorax_source = untar_data(URLs.SIIM_SMALL) . # reading the dcm files items = get_dicom_files(pneumothorax_source/f&quot;train/&quot;) . # lets read the dcm file for patient 11 patient = 11 xray_sample = items[patient].dcmread() . # lets take a look at the DICOM metafile xray_sample . Dataset.file_meta - (0002, 0000) File Meta Information Group Length UL: 202 (0002, 0001) File Meta Information Version OB: b&#39; x00 x01&#39; (0002, 0002) Media Storage SOP Class UID UI: Secondary Capture Image Storage (0002, 0003) Media Storage SOP Instance UID UI: 1.2.276.0.7230010.3.1.4.8323329.10731.1517875225.339875 (0002, 0010) Transfer Syntax UID UI: JPEG Baseline (Process 1) (0002, 0012) Implementation Class UID UI: 1.2.276.0.7230010.3.0.3.6.0 (0002, 0013) Implementation Version Name SH: &#39;OFFIS_DCMTK_360&#39; - (0008, 0005) Specific Character Set CS: &#39;ISO_IR 100&#39; (0008, 0016) SOP Class UID UI: Secondary Capture Image Storage (0008, 0018) SOP Instance UID UI: 1.2.276.0.7230010.3.1.4.8323329.10731.1517875225.339875 (0008, 0020) Study Date DA: &#39;19010101&#39; (0008, 0030) Study Time TM: &#39;000000.00&#39; (0008, 0050) Accession Number SH: &#39;&#39; (0008, 0060) Modality CS: &#39;CR&#39; (0008, 0064) Conversion Type CS: &#39;WSD&#39; (0008, 0090) Referring Physician&#39;s Name PN: &#39;&#39; (0008, 103e) Series Description LO: &#39;view: PA&#39; (0010, 0010) Patient&#39;s Name PN: &#39;8d26a8e1-0913-4952-b5c9-3dd09b3c8925&#39; (0010, 0020) Patient ID LO: &#39;8d26a8e1-0913-4952-b5c9-3dd09b3c8925&#39; (0010, 0030) Patient&#39;s Birth Date DA: &#39;&#39; (0010, 0040) Patient&#39;s Sex CS: &#39;F&#39; (0010, 1010) Patient&#39;s Age AS: &#39;67&#39; (0018, 0015) Body Part Examined CS: &#39;CHEST&#39; (0018, 5101) View Position CS: &#39;PA&#39; (0020, 000d) Study Instance UID UI: 1.2.276.0.7230010.3.1.2.8323329.10731.1517875225.339874 (0020, 000e) Series Instance UID UI: 1.2.276.0.7230010.3.1.3.8323329.10731.1517875225.339873 (0020, 0010) Study ID SH: &#39;&#39; (0020, 0011) Series Number IS: &#34;1&#34; (0020, 0013) Instance Number IS: &#34;1&#34; (0020, 0020) Patient Orientation CS: &#39;&#39; (0028, 0002) Samples per Pixel US: 1 (0028, 0004) Photometric Interpretation CS: &#39;MONOCHROME2&#39; (0028, 0010) Rows US: 1024 (0028, 0011) Columns US: 1024 (0028, 0030) Pixel Spacing DS: [0.14300000000000002, 0.14300000000000002] (0028, 0100) Bits Allocated US: 8 (0028, 0101) Bits Stored US: 8 (0028, 0102) High Bit US: 7 (0028, 0103) Pixel Representation US: 0 (0028, 2110) Lossy Image Compression CS: &#39;01&#39; (0028, 2114) Lossy Image Compression Method CS: &#39;ISO_10918_1&#39; (7fe0, 0010) Pixel Data OB: Array of 163378 elements . As can be seen, the dataset file has many rows. Each row contains a data element. . An example of a data element is (0028, 0010) Rows US: 1024 . Let&#39;s break down the data element . (0028, 0010) is the tag. There are two parts - Group (0028) and Element (0010). From our example of patient 11 above, we can see that Group 0010 groups all patient details. . In our data element example, the tag is followed by Rows which describes the data element. Following the tag and its description, the next value is Value Representation (VR) which describes the data type of the data element. In our example, the VR is US which means Unsigned Short. The VR is then followed by Value Length. In our example, there are 1024 rows. . There are 1,000s of data elements in the DICOM. In this, we will focus on the 0028 group which describes different image/pixel related attributes and (7fe0, 0010) which describes and contains the pixel data. . (0028, 0002) Samples per pixel - This indicates if the image is grayscale (1) or RGB (3). In our example, we have a grayscale image. | (0028, 0004) Photometric Interpretation - describes the color space of our image. Some of the possible values are - MONOCHROME, MONOCHROME2, PALETTE COLOR, RGB. In our case it is MONOCHROME2 where low values are dark and high values are bright. It is the opposite in MONOCHROME. PALETTE COLOR contains a color image with a single sample per pixel. RGB describes red, green and blue image planes. For RGB, samples per pixel would be 3. | (0028, 0010) Rows - describes the number of rows in the image. In our example, there are 1024 rows. | (0028, 0011) Columns - describes the number of columns in the image. In our example, there are 1024 columns. | (0028, 0030) Pixel Spacing - describes the distance between centers of two neighbouring pixels. In our example [0.19431099999999998, 0.19431099999999998], the first number is the Row Spacing, the second number is the Column Spacing. | (0028, 0100) Bits Allocated - Number of bits allocated for each pixel sample. | (0028, 0101) Bits Stored - Number of bits stored for each pixel sample. A 8 bits image would have pixel value between 0-255. | (0028, 0102) High Bit - Most significant bit for pixel sample data. Each sample shall have the same high bit. | (0028, 0103) Pixel Representation - can either be unsigned(0) or signed(1). If you are like me and need a refresher on signed vs unsigned integer, here is a link | (0028, 2110) Lossy Image Compression - Specifies whether an Image has undergone lossy compression. 00 image has not been subjected to lossy compression. 01 image has been subjected to lossy compression. lossy compression or irreversible compression is the class of data encoding methods that uses inexact approximations and partial data discarding to represent the content. These techniques are used to reduce data size for storing, handling, and transmitting content. | (0028, 2114) Lossy Image Compression Method - the methods used in Lossy Image compression. ISO_10918_1 : JPEG Lossy Compression, ISO_14495_1 : JPEG-LS Near-lossless Compression, ISO_15444_1 : JPEG 2000 Irreversible Compression : ISO_13818_2 MPEG2 Compression, ISO_14496_10 : MPEG-4 AVC/H.264 Compression, ISO_23008_2 : HEVC/H.265 Lossy Compression. In our example, it is ISO_10918_1 which is JPEG Lossy Compression. | (7fe0, 0010) Pixel Data - an array of pixel data. Data type is OB. Let&#39;s take a look below. | . Apart from the above, let&#39;s also understand the below . (0008,0060) Modality - Type of equipment that originally acquired the data used to create the images in this Series.For all the different values, refer here. Some examples are CT : Computed Tomography, CR: Computed Radiography, DX : Digital Radiography, ES : Endoscopy, IVUS : Intravascular Ultrasound | . Let&#39;s take a look at a sample of PixelData . xray_sample.PixelData[:200] . b&#39; xfe xff x00 xe0 x00 x00 x00 x00 xfe xff x00 xe0&#34;~ x02 x00 xff xd8 xff xdb x00C x00 x03 x02 x02 x02 x02 x02 x03 x02 x02 x02 x03 x03 x03 x03 x04 x06 x04 x04 x04 x04 x04 x08 x06 x06 x05 x06 t x08 n n t x08 t t n x0c x0f x0c n x0b x0e x0b t t r x11 r x0e x0f x10 x10 x11 x10 n x0c x12 x13 x12 x10 x13 x0f x10 x10 x10 xff xc0 x00 x0b x08 x04 x00 x04 x00 x01 x01 x11 x00 xff xc4 x00 x1d x00 x00 x02 x03 x01 x01 x01 x01 x01 x00 x00 x00 x00 x00 x00 x00 x00 x04 x05 x00 x03 x06 x02 x07 x01 x08 t xff xc4 x00J x10 x00 x02 x01 x03 x03 x02 x04 x03 x06 x06 x01 x02 x05 x02 x00 x0f x01 x02 x11 x00 x03! x04 x121 x05A x13&#34;Qa x06q x81 x142 x91 xa1 xb1 xf0 x07#B xc1 xd1 xe1 xf1 x15R x08$3br x16C%4Sc tD x82 x92&#39; . As the raw PixelData are complex. Let&#39;s use .pixel_array to read the data in a more familiar format. . xray_sample.pixel_array . array([[ 0, 0, 0, ..., 233, 248, 153], [ 0, 0, 0, ..., 226, 241, 151], [ 0, 0, 0, ..., 208, 223, 140], ..., [ 1, 1, 1, ..., 2, 2, 0], [ 1, 1, 1, ..., 2, 2, 0], [ 1, 1, 1, ..., 2, 2, 0]], dtype=uint8) . Let&#39;s use .show to show the image . xray_sample.show() . fastai provides the following function to create dataframe from the dicom files. Apart from reading the DICOM file, it also calculate summary statistics of the image pixels (mean/min/max/std) when px_summ is set to True . dicom_dataframe = pd.DataFrame.from_dicoms(items, px_summ=True) dicom_dataframe[:5] . SpecificCharacterSet SOPClassUID SOPInstanceUID StudyDate StudyTime AccessionNumber Modality ConversionType ReferringPhysicianName SeriesDescription PatientName PatientID PatientBirthDate PatientSex PatientAge BodyPartExamined ViewPosition StudyInstanceUID SeriesInstanceUID StudyID SeriesNumber InstanceNumber PatientOrientation SamplesPerPixel PhotometricInterpretation Rows Columns PixelSpacing BitsAllocated BitsStored HighBit PixelRepresentation LossyImageCompression LossyImageCompressionMethod fname MultiPixelSpacing PixelSpacing1 img_min img_max img_mean img_std img_pct_window . 0 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.11405.1517875232.807474 | 19010101 | 000000.00 | | CR | WSD | | view: PA | (8, 1, 5, 5, 1, 5, 9, 8, -, a, b, 5, 1, -, 4, e, 5, 7, -, a, 4, d, e, -, 8, d, 0, b, 0, 3, 9, 9, 3, c, 5, 5) | 81551598-ab51-4e57-a4de-8d0b03993c55 | | F | 66 | CHEST | PA | 1.2.276.0.7230010.3.1.2.8323329.11405.1517875232.807473 | 1.2.276.0.7230010.3.1.3.8323329.11405.1517875232.807472 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.171000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000018.dcm | 1 | 0.171000 | 0 | 252 | 150.097208 | 59.213376 | 0.136433 | . 1 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.2126.1517875171.269922 | 19010101 | 000000.00 | | CR | WSD | | view: AP | (4, 4, 2, 7, 4, 9, d, 8, -, 8, 2, 7, 9, -, 4, 3, e, d, -, a, 2, a, 3, -, 8, 5, 1, 8, 3, 9, 3, e, 0, 0, e, b) | 442749d8-8279-43ed-a2a3-8518393e00eb | | M | 28 | CHEST | AP | 1.2.276.0.7230010.3.1.2.8323329.2126.1517875171.269921 | 1.2.276.0.7230010.3.1.3.8323329.2126.1517875171.269920 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.139000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000071.dcm | 1 | 0.139000 | 0 | 255 | 144.198807 | 54.626554 | 0.071772 | . 2 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.31988.1517875157.881392 | 19010101 | 000000.00 | | CR | WSD | | view: PA | (6, 3, a, 4, 0, 2, a, 4, -, 8, 2, 8, c, -, 4, c, c, 4, -, 9, 7, 6, 0, -, 4, 3, 2, 6, 0, 2, 1, 6, 2, 4, 2, 5) | 63a402a4-828c-4cc4-9760-432602162425 | | M | 65 | CHEST | PA | 1.2.276.0.7230010.3.1.2.8323329.31988.1517875157.881391 | 1.2.276.0.7230010.3.1.3.8323329.31988.1517875157.881390 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.168000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000037.dcm | 1 | 0.168000 | 0 | 255 | 176.959857 | 51.083963 | 0.036505 | . 3 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.32498.1517875160.877894 | 19010101 | 000000.00 | | CR | WSD | | view: AP | (c, f, 2, a, 4, 6, 7, f, -, e, b, 2, 0, -, 4, 5, 1, 6, -, 8, f, 3, 6, -, 3, 6, d, e, 2, d, 4, 5, 4, 5, 4, e) | cf2a467f-eb20-4516-8f36-36de2d45454e | | F | 48 | CHEST | AP | 1.2.276.0.7230010.3.1.2.8323329.32498.1517875160.877893 | 1.2.276.0.7230010.3.1.3.8323329.32498.1517875160.877892 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.168000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000135.dcm | 1 | 0.168000 | 0 | 255 | 107.833434 | 65.194095 | 0.241501 | . 4 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.2633.1517875173.805125 | 19010101 | 000000.00 | | CR | WSD | | view: PA | (9, 4, 3, a, 4, e, 9, 3, -, d, 0, 5, 8, -, 4, 8, 8, 4, -, b, c, 6, 1, -, 7, b, 6, f, b, b, 2, 8, 8, c, 8, 5) | 943a4e93-d058-4884-bc61-7b6fbb288c85 | | F | 40 | CHEST | PA | 1.2.276.0.7230010.3.1.2.8323329.2633.1517875173.805124 | 1.2.276.0.7230010.3.1.3.8323329.2633.1517875173.805123 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.194311 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000103.dcm | 1 | 0.194311 | 0 | 255 | 96.317883 | 43.559422 | 0.267043 | . We have 250 samples which wouldn&#39;t be sufficient to build anything of significance. We will use it to understand the DICOM data and to learn how fastai can be used to work with medical images. . len(dicom_dataframe) . 250 . Let&#39;s take a look at the different columns. img_min, img_max, img_mean, img_std, img_pct_window are calculated by from_dicom fastai function. . dicom_dataframe.columns . Index([&#39;SpecificCharacterSet&#39;, &#39;SOPClassUID&#39;, &#39;SOPInstanceUID&#39;, &#39;StudyDate&#39;, &#39;StudyTime&#39;, &#39;AccessionNumber&#39;, &#39;Modality&#39;, &#39;ConversionType&#39;, &#39;ReferringPhysicianName&#39;, &#39;SeriesDescription&#39;, &#39;PatientName&#39;, &#39;PatientID&#39;, &#39;PatientBirthDate&#39;, &#39;PatientSex&#39;, &#39;PatientAge&#39;, &#39;BodyPartExamined&#39;, &#39;ViewPosition&#39;, &#39;StudyInstanceUID&#39;, &#39;SeriesInstanceUID&#39;, &#39;StudyID&#39;, &#39;SeriesNumber&#39;, &#39;InstanceNumber&#39;, &#39;PatientOrientation&#39;, &#39;SamplesPerPixel&#39;, &#39;PhotometricInterpretation&#39;, &#39;Rows&#39;, &#39;Columns&#39;, &#39;PixelSpacing&#39;, &#39;BitsAllocated&#39;, &#39;BitsStored&#39;, &#39;HighBit&#39;, &#39;PixelRepresentation&#39;, &#39;LossyImageCompression&#39;, &#39;LossyImageCompressionMethod&#39;, &#39;fname&#39;, &#39;MultiPixelSpacing&#39;, &#39;PixelSpacing1&#39;, &#39;img_min&#39;, &#39;img_max&#39;, &#39;img_mean&#39;, &#39;img_std&#39;, &#39;img_pct_window&#39;], dtype=&#39;object&#39;) . We have CR as the Modelity which is Computed Radiograpy . dicom_dataframe[&#39;Modality&#39;].unique() . array([&#39;CR&#39;], dtype=object) . We can see the age distribution of the patient. . plt.style.use(&#39;seaborn&#39;) dicom_dataframe[&#39;PatientAge&#39;].astype(int).hist(bins=10) plt.xlabel(&#39;Age&#39;) plt.ylabel(&#39;Frequency&#39;) . Text(0, 0.5, &#39;Frequency&#39;) . We have an equal number of M and F . dicom_dataframe[&#39;PatientSex&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa9c9cebe10&gt; . Building a classifier with fastai . Next, let&#39;s use fastai.medical to build a simple classifier. Again, we only have 250 samples which won&#39;t be enough to build anything of significance. The goal is to show how we can use fastai.medical to work with medical images. . Let&#39;s get the different folders/files. . pneumothorax_source.ls() . (#2) [Path(&#39;/root/.fastai/data/siim_small/train&#39;),Path(&#39;/root/.fastai/data/siim_small/labels.csv&#39;)] . Let&#39;s read the labels.csv and see what is in it . train = pd.read_csv(pneumothorax_source/&#39;labels.csv&#39;) . Let&#39;s build a simple dataloader . pneumothorax = DataBlock(blocks=(ImageBlock(cls=PILDicom), CategoryBlock), get_x=lambda x:pneumothorax_source/f&quot;{x[0]}&quot;, get_y=lambda x:x[1], splitter=RandomSplitter(), batch_tfms=aug_transforms(size=400)) dls = pneumothorax.dataloaders(train.values, bs=8) . Once, we have made the dataloader, we can take a look at a batch. . dls.show_batch(max_n=8) . Let&#39;s use fastai&#39;s xrestnet model - xresnet is based on &quot;Bag of Tricks for ResNet&quot; paper. We also use Mish activation instead of the usual ReLU and we will use self-attention. . model = xresnet50(pretrained=False, act_cls=Mish, sa=True, n_out=2) . model[0][0] . Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) . # Here, we will set the first layer to accept single channel image model[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) . For the optimizer, we will use ranger which uses RAdam and Lookahead. . learn = Learner(dls, model=model, loss_func=LabelSmoothingCrossEntropy(), metrics= accuracy, opt_func=ranger) . learn.lr_find() . SuggestedLRs(lr_min=3.311311302240938e-05, lr_steep=0.0012022644514217973) . learn.fit_flat_cos(5, 5e-4) . epoch train_loss valid_loss accuracy time . 0 | 0.635151 | 0.681318 | 0.680000 | 00:13 | . 1 | 0.629701 | 0.652602 | 0.680000 | 00:12 | . 2 | 0.614139 | 0.611938 | 0.720000 | 00:13 | . 3 | 0.601922 | 0.743685 | 0.640000 | 00:12 | . 4 | 0.596375 | 0.660376 | 0.700000 | 00:12 | . Interpreting the classifier with fastai . Among the many cool things fastai provides, interpretation is one. Let&#39;s take a look at the classifier and see how well our classifier is doing. . learn.show_results(max_n=8) . # Let&#39;s initiate a ClassificationInterpretation interp = ClassificationInterpretation.from_learner(learn) . Let&#39;s take a look at our top_losses. Our classifier confuses No Pneumothorax for Pneumothorax. This is likely because of the lack of training data. Again, the goal here is to understand how we can use fastai and its many tools. . interp.plot_top_losses(6, figsize=(14,8)) . As expected, there is a lot of False Negative (predicts &quot;No Pneumothorax&quot; when it is &quot;Pneumothorax&quot;) . interp.plot_confusion_matrix(figsize=(7,7)) . CAM and GradCAM . CAM - Class Activation Map . . credits: https://docs.paperspace.com/machine-learning/wiki/interpretability . Interpretability or explainability is the degree to which a model&#39;s prediction/decision can be explained in human terms. This is a huge area of research as often ML models are said to be a black-box with no interpretability. There are certain tools being developed to address this area. Among those tools are CAM and GradCAM. . Class Activation Map (CAM) uses the activation of the last convolution layer and the predictions of the last layer to plot heatmap visualization. The visualization gives an idea of why the model made its decision. In medical imaging, this sort of heatmap visualization could augment radiologists and other doctors apart from doing the classification. Fastbook has a chapter dedicated to CAM and GradCAM which can be found here. . Let&#39;s see how we can make use of CAM. . Below, we define a hook class. Hooks are similar to callbacks and they let us inject codes into forward and backward calculation. . class Hook(): def hook_func(self, m, i, o): self.stored = o.detach().clone() . Let&#39;s define the path for no_pneumothorax and pneumothorax class . nopneumo = (pneumothorax_source/&#39;train&#39;).ls()[0].ls() pneumo = (pneumothorax_source/&#39;train&#39;).ls()[1].ls() . Then, we initiate the Hook class and use the register_forward_hook to attach the hook class to the forward function. learn.model[-5] would access the whole xresnet model without the head and register_forward_hook would be able to attach our hook to the last convolution layer. . hook_output = Hook() hook = learn.model[-5].register_forward_hook(hook_output.hook_func) . Let&#39;s define a function to grab a sampel of image either from nopneumo or pneumo folders. . def grab_x(path, patient): x = first(dls.test_dl([path[patient]])) return x[0] . Let&#39;s define a function to get the CAM map. As you can see we make use of the einsum function. It is awesome funtion and here is one of my fav video on this topic. . def get_cammap(x): with torch.no_grad(): output = learn.model.eval()(x) act = hook_output.stored[0] print(F.softmax(output, dim=-1)) cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[-1].weight, act) hook.remove() return cam_map . Then, Let&#39;s define a function to plot two images - left_image = input image and right_image = input image superimposed by the CAM activation heatmap. idx=0 to see nopneumo class activation and idx=1 to see pneumo class activation. . def plot_cam(x, cls, cam_map, img_size=400): x_dec = TensorDicom(dls.train.decode((x,))[0][0]) _,ax = plt.subplots(1,2, figsize=(15,10)) x_dec.show(ctx=ax[0]) x_dec.show(ctx=ax[1]) ax[1].imshow(cam_map[cls].detach().cpu(), alpha=0.6, extent=(0,img_size,img_size,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . Let&#39;s write a function to wrap everything. . def get_plot_cam_image(path, patient, cls): x = grab_x(path, patient) cam_map = get_cammap(x) plot_cam(x, cls, cam_map, img_size=400) . Below, is an example for No pneumothorax. Areas in bright yellow/orange corresponds to high activations while areas in purple corresponds to low activations. Unfortunately, our classifier hasn&#39;t learnt much to show this. Hence, lets see pic from fastbook. . . The activation map on the cat allows one to peek into model&#39;s &#39;reasons&#39; for its prediction. In medical imaging, this might highlight tumors and other such abnormalities that the radiologists could further scrutanize. . get_plot_image(nopneumo, 5, 0) . tensor([[0.8117, 0.1883]], device=&#39;cuda:0&#39;) . Below is the same image but observing for class=1 or for Pneumothorax. Some of the bright orange are around the lungs as oppose to above where for No Pneumothorax the lungs appeared purple highlighting no activation around the lungs. . get_plot_image(nopneumo, 5, 1) . tensor([[0.8117, 0.1883]], device=&#39;cuda:0&#39;) . GradCAM . Having seen how to use CAM in fastai. Let&#39;s take a look at GradCAM. . GradCAM is similar to CAM except in GradCAM we make use of the gradient to plot the visualization. Because we use gradient, we are able to plot the visualization for the earlier conv layers too. With CAM, we were only able to observe the visualization for the final conv layer because once we obtained the activation of the conv layer, we need to multiply by the last weight matrix.This method only works for the final conv layer. This variant was introduced in the paper - &quot;Grad-CAM: Why Did You Say That? Visual Explanations from Deep Networks via Gradient-based Localization&quot; in 2016. . # A hook to store the output of a layer class Hook(): def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func) def hook_func(self, m, i, o): self.stored = o.detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() # A hook to store the grad of a layer class HookBwd(): def __init__(self, m): self.hook = m.register_backward_hook(self.hook_func) def hook_func(self, m, gi, go): self.stored = go[0].detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . def get_gradcammap(x, cls, model_layer): with HookBwd(model_layer) as hookg: with Hook(model_layer) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) return cam_map . def plot_gcam(x, img_size=400): x_dec = TensorDicom(dls.train.decode((x,))[0][0]) _,ax = plt.subplots(1,2, figsize=(15,10)) x_dec.show(ctx=ax[0]) x_dec.show(ctx=ax[1]) ax[1].imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,img_size,img_size,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . def get_plot_gcam_image(path, patient, cls, layer=-5): x = grab_x(path, patient) cam_map = get_gradcammap(x, cls, layer) plot_gcam(x) . get_plot_gcam_image(nopneumo, 7, cls=0, layer=learn.model[-5]) . get_plot_gcam_image(nopneumo, 7, cls=1, layer=learn.model[-5]) . get_plot_gcam_image(nopneumo, 7, cls=0, layer=learn.model[-5]) . References . https://docs.fast.ai/tutorial.medical_imaging.html . http://dicomiseasy.blogspot.com/2011/10/introduction-to-dicom-chapter-1.html . http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.3.html#sect_C.7.6.3.1.4 . https://dicom.innolitics.com/ciods/ct-image/image-plane/00280030#:~:text=All%20pixel%20spacing%20related%20Attributes,adjacent%20rows%2C%20or%20vertical%20spacing. .",
            "url": "https://moarshy.github.io/blogs/fastai/medical/interpretability/2021/02/07/whatisdicom.html",
            "relUrl": "/fastai/medical/interpretability/2021/02/07/whatisdicom.html",
            "date": " • Feb 7, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Image Classification Techniques",
            "content": "This is going to be my first blog. I would like to start by stating the motivation for starting this. The main reason for starting is because people I respect in the deep learning (DL) community have all advocated for blogging as part of the learning process. Hence, I am hoping to articulate my learning and understanding through these blogs. It also means I am open to anyone correcting my understanding as well as to add to my current understanding. My blog is mostly going to be around DL and fastai. This year one of my goals is to be around the fastai community so I could learn from the amazing people and the conversation that takes place there. . In this blog, we will go through methods/techniques that help in image classification tasks. The following are the techniques I have been learning and as much as I can I would reference where I learnt the techniques from so anyone could learn from the source. The examples/codes will be using the fastai library. . Image classification is possibly the first task one would encounter when learning DL. Image classification is a computer vision task where a model classifies an image. For example, a cat or dog classifier classifies whether an image is a cat or a dog. . The types of image classifiction tasks . binary image classification - a task in which the model has to predict between two classes (eg. cat or dog) | multi-class image classification - a classification task in which the model has to predict between n-classes (eg. cat, dog, horse or bear) | multi-label image classification - a classification task in which the model has to predict between n-classes and in each prediction there can be one or more than one predictions. (eg. cat and dog) | Throughout this blog we will make use of the Plant Pathology dataset from Kaggle to understand how the different techniques can be applied. . So first, lets understand our dataset. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . !pip uninstall fastai -q -y !pip install fastai --upgrade -q . |████████████████████████████████| 194kB 17.2MB/s |████████████████████████████████| 61kB 9.7MB/s . from fastai.vision.all import * from sklearn.model_selection import StratifiedKFold . SEED=101 set_seed(SEED) path = Path(&#39;/content/drive/MyDrive/colab_notebooks/fastai/plant_pathology/data&#39;) train = pd.read_csv(path/&#39;train.csv&#39;) train.head(3) . image_id healthy multiple_diseases rust scab . 0 Train_0 | 0 | 0 | 0 | 1 | . 1 Train_1 | 0 | 1 | 0 | 0 | . 2 Train_2 | 1 | 0 | 0 | 0 | . Let&#39;s look at the data. As can see from the above, our train.csv contains the image_id and the labels. There are four classes - healthy, multiple_diseases, rust and scab . train[&#39;labels&#39;] = train.iloc[:, 1:].idxmax(1) train[&#39;labels&#39;].value_counts(), len(train) . (rust 622 scab 592 healthy 516 multiple_diseases 91 Name: labels, dtype: int64, 1821) . In total, there are 1,821 train images. Except for the multiple_diseases class, all other classes have similar number of training examples. One of the problems with this dataset is the relatively low number of multiple_diseases examples in the dataset. Later, we will see how we can use oversampling to help with this. . Now let&#39;s start with the first technique. . 1. k-fold crossvalidation . Oftentimes, training data is scarce and you might want to use all the given data in training but because in crossvalidation (train-validation split) some percentage of data is kept for validation, and that data becomes unavailable for training our model. This is where k-fold crossvalidation could be useful. How does this work? . Create k folds of validation data | Train k models using different validation set each time | During inference, make prediction on all k models and average the results | This way not only we will be ensembling k models during inference, we would also use all the data in the training process. . Let&#39;s see how this is done. . N_FOLDS = 3 train[&#39;fold&#39;] = -1 strat_kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True) for i, (_, test_index) in enumerate(strat_kfold.split(train.image_id.values, train[&#39;labels&#39;].values)): train.iloc[test_index, -1] = i train[&#39;fold&#39;] = train[&#39;fold&#39;].astype(&#39;int&#39;) . train.head(5) . image_id healthy multiple_diseases rust scab labels fold . 0 Train_0 | 0 | 0 | 0 | 1 | scab | 2 | . 1 Train_1 | 0 | 1 | 0 | 0 | multiple_diseases | 2 | . 2 Train_2 | 1 | 0 | 0 | 0 | healthy | 0 | . 3 Train_3 | 0 | 0 | 1 | 0 | rust | 1 | . 4 Train_4 | 1 | 0 | 0 | 0 | healthy | 2 | . We have 3 folds (or 3 differenct validation sets) . train[&#39;fold&#39;].value_counts() . 2 607 1 607 0 607 Name: fold, dtype: int64 . train.groupby([&#39;fold&#39;, &#39;labels&#39;]).size() . fold labels 0 healthy 172 multiple_diseases 30 rust 207 scab 198 1 healthy 172 multiple_diseases 31 rust 207 scab 197 2 healthy 172 multiple_diseases 30 rust 208 scab 197 dtype: int64 . So we have created three validation sets with each sets having 607 samples. Also because we used stratified k-fold, the different classes in each validation sets are about the same. Now we are ready to proceed with training our k models. . 2. Oversampling and undersampling . As we saw earlier, multiple_diseases class has only 90 samples as compared to others averaging around 500+. This might disadvantage the multiple_diseases class as the model might learn to predict multiple_diseases less often to improve the metrics. . In such scenarios, oversampling can be used. Oversampling is nothing but copy-pasting the same training data of a certain class to increase its numbers. . Let&#39;s see how this is done. . def oversampling(df, fold, col2os=&#39;multiple_diseases&#39;, oversampling=3): train_df_no_val = df[df[&#39;fold&#39;] != {fold}] #training set train_df_just_val = df[df[&#39;fold&#39;] == {fold}] #validation set #we only want oversample the multiple_disease class in the training set train_df_bal = pd.concat( [train_df_no_val[train_df_no_val[&#39;labels&#39;] != col2os], train_df_just_val] + [train_df_no_val[train_df_no_val[&#39;labels&#39;] == col2os]] * oversampling ).sample(frac=1.0, random_state=SEED).reset_index(drop=True) train_df_bal.reset_index(drop=True) return train_df_bal . train_os = oversampling(train, 0) . We have more data in train_os where we have used oversampling of multiple_diseases class . len(train), len(train_os) . (1821, 2003) . train_fold0 = train[train[&#39;fold&#39;] != 0] train_os_fold0 = train_os[train_os[&#39;fold&#39;] != 0] . (print(&#39;train without oversamplig&#39;, &#39; n n&#39;, train_fold0[&#39;labels&#39;].value_counts(), &#39; n n&#39;, &#39;train with oversampling&#39;, &#39; n n&#39;, train_os_fold0[&#39;labels&#39;].value_counts(), sep=&quot;&quot;)) . train without oversamplig rust 415 scab 394 healthy 344 multiple_diseases 61 Name: labels, dtype: int64 train with oversampling rust 415 scab 394 healthy 344 multiple_diseases 183 Name: labels, dtype: int64 . As we can see we have 3x our multiple_diseases class after using oversampling. Samples of other classes stay the same. Oversampling as well as its counterpart undersampling can be useful in balancing the sample size of different classes in the dataset. This allows the model to be trained with less bias towards any of the classes. . 3. Techniques from fastbook Chapter 7 . Fastbook is an amazing resource to learn DL and it is my go to resource. In Chapter 7, advanced techniques for training an image classification model are introduced. Let&#39;s see what these techniques are. . Normalization | Data augmentation including MixUp (CutMix) | Progressive resizing | Test time augmentation | Normalization . We know that having the mean and std of our input data around 0 and 1 helps the model train more efficiently and helps in generalization. Hence, normalization is almost a default technique these days. . Generally, when we train image classification we start by transfer learning. These models would have been generally trained using the ImageNet dataset. Hence, when we normalize our data we use the mean and std of ImageNet dataset to normalize our data. . If we are training from scratch, it&#39;s recommended to calculate the mean and std of the dataset for the 3-channels and use that to normalize the data. Also, during inference, the test data should be normalized using whatever stats that were used to normalize during the training. . Doing this in fastai is very easy. Let&#39;s take a look. . Let&#39;s write a function to make our dataloader . def get_dls(fold, df, img_sz=224): datablock = DataBlock( blocks=(ImageBlock, CategoryBlock()), getters=[ ColReader(&#39;image_id&#39;, pref=path/&#39;images&#39;, suff=&#39;.jpg&#39;), ColReader(&#39;labels&#39;) ], splitter=IndexSplitter(df.loc[df.fold==fold].index), item_tfms=Resize(img_sz), ) return datablock.dataloaders(source=df, bs=32) . dls = get_dls(0, train_os) x, y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.3879, 0.5049, 0.2897], device=&#39;cuda:0&#39;), TensorImage([0.1893, 0.1820, 0.1703], device=&#39;cuda:0&#39;)) . Our mean and std are nowhere near 0 and 1. . Below is how we could calculate the mean and std of our dataset. . Note: This would only use the train dataset. Hence, a more stringent way would be to use all images and calculate the mean and std. . m,s = [0., 0., 0.], [0., 0., 0.] count = 0 for x, y in next(iter(dls)): m += np.array(x.mean(dim=[0,2,3]).cpu()) s += np.array(x.std(dim=[0,2,3]).cpu()) count += 1 . m/count , s/count . (array([0.39644337, 0.51481164, 0.30797708]), array([0.19386025, 0.17980762, 0.17882748])) . Let&#39;s modify our get_dls function slightly in the batch_tfms argument. We have normalize and are using imagenet_stats to normalize the data. Let&#39;s see what is imagenet_stats first and see how this has changed our mean and std. . def get_dls(fold, df, img_sz): datablock = DataBlock( blocks=(ImageBlock, CategoryBlock()), getters=[ ColReader(&#39;image_id&#39;, pref=path/&#39;images&#39;, suff=&#39;.jpg&#39;), ColReader(&#39;labels&#39;) ], splitter=IndexSplitter(df.loc[df.fold==fold].index), item_tfms=Resize(img_sz), batch_tfms=[Normalize.from_stats(*imagenet_stats)] ) return datablock.dataloaders(source=df, bs=32) . As can be seen below, imagenet_stats is a tuple that has the mean and std for the three channels. If you have the stats for your dataset, you could also pass it similarly to normalize the data. . imagenet_stats . ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) . dls = get_dls(0, train_os, 224) x, y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([-0.3794, 0.2188, -0.4551], device=&#39;cuda:0&#39;), TensorImage([0.9271, 0.8399, 0.8564], device=&#39;cuda:0&#39;)) . The mean and std following normalization is much closer to 0 and 1. . Data augmentation, MixUp and CutMix . Data augmentation is a well known technique to improve image classification. Fastai provides many of these data augmentation tools and they can be easily applied while creating the dataloaders like how we normalized earlier. Data augmentation can be passed as an argument in either item_tfms or batch_tfms while creating our datablock. The difference between the both is that the former make use of CPU while the latter make use of GPU. Hence, batch_tfms is the preferred method to carry out most of the augmentation. . Data augmentation essentially allows us to enlarge our dataset size without getting new data. Data augmentation essentially uses synthetic data manipulation to create new images/training data. . Let&#39;s use this image to see some examples of the many data augmentation that comes with fastai. . img = PILImage(PILImage.create((path/&#39;images&#39;).ls()[SEED]).resize((600,400))) img . FlipItem flips the image at the given probability p . _,axs = subplots(2, 4) for ax in axs.flatten(): show_image(FlipItem(p=0.5)(img, split_idx=0), ctx=ax) . Another technique is dihedral, let&#39;s see what it does . _,axs = subplots(2, 4) for ax in axs.flatten(): show_image(DihedralItem(p=1.)(img, split_idx=0), ctx=ax) . RandomCropping the image at given size . _,axs = subplots(2, 4) for ax in axs.flatten(): show_image(RandomCrop(224)(img, split_idx=0), ctx=ax) . And aug_transforms which is an &quot;Utility func to easily create a list of flip, rotate, zoom, warp, lighting transforms.&quot; . timg = TensorImage(array(img)).permute(2,0,1).float()/255. def _batch_ex(bs): return TensorImage(timg[None].expand(bs, *timg.shape).clone()) . Each image is different although the input image was the same . tfms = aug_transforms(pad_mode=&#39;zeros&#39;, mult=2, min_scale=0.5) y = _batch_ex(9) for t in tfms: y = t(y, split_idx=0) _,axs = plt.subplots(2,3, figsize=(12,10)) for i,ax in enumerate(axs.flatten()): show_image(y[i], ctx=ax) . MixUp . MixUp is a data augmentation technique that was introduced in 2018 in this paper. So what happens during a MixUp? . for a training image image_1, a second image image_2 is randomly selected | new_image is created following this formula where alpha is a constant between 0. and 1.0 that is used to mix the two images | new_image = alpha * image_1 + (1-alpha) * image_2 . similarly, the targets of image_1 and image_2 are blended to create new_target | For example, let&#39;s assume we are training a 4-class model and the &gt;one-hot-encode for image_1 is [0., 0., 1., 0.] and image_2 is [0., 0., &gt;0., 1.]. Also, let&#39;s assume alpha is 0.3. The target for our new_image is &gt;[0., 0., 0.3, 0.7]. . new_target = 0.3 * [0., 0., 1., 0.] + (1-0.3) * [0., 0., 0., 1.] . With this, now, we have completely new image for training. | Let&#39;s use the codes from fastai docs to see how our MixUp images look . mixup = MixUp(1.) with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=mixup) as learn: learn.epoch,learn.training = 0,True learn.dl = dls.train b = dls.one_batch() learn._split(b) learn(&#39;before_batch&#39;) _,axs = plt.subplots(3,3, figsize=(9,9)) dls.show_batch(b=(mixup.x,mixup.y), ctxs=axs.flatten()) . epoch train_loss valid_loss time . 0 | 00:01 | . As can be seen, some of our images above look a bit smeared/odd that is because of mixup. As can be seen, using MixUp with fastai is relatively easy. It is passed as a callback argument when we initiate a Learner. It can also be passed as a cbs in fit_one_cycle. . learn.fit_one_cycle(3, cbs=MixUp(1.0)) . Note: the alpha we passed when we initiate the mixup will be used to generate a distribution the size of batch_size hence the alpha varies from one image to another. Below an example of generating the alpha distribution . torch.distributions.beta.Beta(tensor(1.), tensor(1.)).sample((10,)) . tensor([0.4794, 0.3758, 0.1914, 0.6586, 0.6198, 0.5889, 0.1123, 0.9081, 0.2395, 0.4103]) . CutMix . Although CutMix was not covered in the book, it has been added to the fastai library. CutMix is similar to MixUp but instead of blending images together, CutMix works by cropping a portion of image_2 and placing it in image_1. CutMix has been shown to work better than MixUp. . . Source: CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features . Let&#39;s see some examples of CutMix in action . cutmix = CutMix(1.) with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=cutmix) as learn: learn.epoch,learn.training = 0,True learn.dl = dls.train b = dls.one_batch() learn._split(b) learn(&#39;before_batch&#39;) _,axs = plt.subplots(3,3, figsize=(9,9)) dls.show_batch(b=(cutmix.x,cutmix.y), ctxs=axs.flatten()) . epoch train_loss valid_loss time . 0 | 00:01 | . Progressive resizing . As stated in the book, progressive resizing gradually uses larger and larger images as we continue our training. This technique is akin to transfer learning. Our model learns on smaller images and as we increase the image size it carries forward what it had learnt in previous training as well as picks up something additional from the larger images. . Test time augmentation . As taken from the fastbook, &quot;test time augmentation (TTA): During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.&quot; . Other things . Different architectures . Generally varying sizes of ResNet would be the first model to try and establish a baseline. After which one could explore other architecture such as efficientnet or the recently released Visual Transformer. . Transfer Learning . In most cases, transfer learning works really well hence it could be the first thing to try for most classification tasks. . 4. Techniques I learned from Zach&#39;s walkwithfastai imagewoof lecture . I highly recommend walkwithfastai course. It is also my go to resource for fastai. In this particular notebook, Zach introduces different techniques that seem to work really well for image classification tasks. Please check the notebook for references and details. Zach also has a lecture using this notebook here. . The notebook introduces the following techniques. . xresnet which is an arch based on the &quot;Bag of Tricks for ResNet&quot; paper | Mish - a new activation function | ranger - a new optimizer that combines RAdam and Lookahead | Self-attention | MaxBlurPool | a different LR scheduler - that uses flatten+anneal scheduling | Label Smoothing Cross Entropy | 5. Softlabeling and progressive label correction . Softlabeling . I came across softlabeling through Isaac Flath&#39;s amazing blog. I think the blog is the best place to get started on softlabeling. . In supervised learning, labels, which are created by humans, could be erroneous. This leads to the labels being &#39;noisy&#39;. This was the case in the Plant Pathology competition and the winner used a similar method (softlabeling) in the winning solution. . How do we deal with such noisy labels? One way is to punish the model less for predicting incorrectly a noisy label. The steps are as follows . Create a k-fold crossvalidation | Train k classifier using different k-fold for sufficient epochs using the noisy labels | Use the classifier to predict on the kth validation set and save the prediction | Upon completion of the above step, you will have two labels - one the noisy label that came with the data label_ori and another predicted by the above classifiers label_pred | Finally, train your actual classifier and this time when labels between label_ori and label_pred differs, adjust the labels using a hyperparameter a | For example, let&#39;s assume we are training a 4-class model and the one-hot-encode for label_ori is [0., 0., 1., 0.] and label_pred is [0., 0., 0., 1.]. Also, let&#39;s assume a is 0.5. Our new_label would be [0., 0., 0.5, 0.5]. By doing this, the model would be punished less for predicting the wrong class as this could be due to noisy labels. . a = 0.5 label_ori = [0., 0., 1., 0.] label_pred = [0., 0., 0., 1.] new_label = a * label_ori + (1-a) * label_pred new_label = [0., 0., 0.5, 0.5] . Progressive Label Correction . I came across this technique in thsi wonderful Kaggle Notebook by Kerem Turgutlu. It is a paper implementation of this paper. . Again, this method works in cases where there are noisy labels in the dataset. This is my understanding of how it is implemented. . During model training, we let a model train normally for a warm_up period. In the above implementation, the warm_up period was 20% of the total iterations. | Once training goes over the warm_up iterations, Progressive Label Correction (PLC) kicks-in | In PLC, after an iteration, mislabeled indexes are identified | Then, we calculate the probabilities of the max prediction class (predicted_probas - the class the model predicted) as well as the actual target class (actual_probas - the class the model should have predicted) of the mislabeled indexes | we check if the absolute difference between predicted_probas and the actual_probas is above a theta value (theta is a hyperparameter we set) | if the difference is higher, then for those mislabeled indexes, we change (‘correct’) the label y to be that predicted by the model | we continue step 2 to step 6 while progressively lowering the theta using a scheduler function (linear scheduler was used in the above notebook). | Let&#39;s take a look what the ProgressiveLabelCorrection callback in the notebook does. . dls = get_dls(0, train_os, 128) . learn = cnn_learner(dls, resnet18, pretrained=True) . Lets assume our learner has been trained for warm_up iterations and see how PLC is applied using this one_batch . learn.fit_one_cycle(1) learn.one_batch(5, learn.dls.one_batch()) . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss time . 0 | 1.573150 | 1.102266 | 02:34 | . . 28.57% [6/21 01:46&lt;04:27 1.5732] Here after an iteration, we check the predicted class and compare it to the target (y) to get the mislabelled indexes . preds_max = learn.pred.argmax(-1) mislabeled_idxs = preds_max != learn.y #so we have 32 samples in each iteration (which is the batch_size), of which 8 are mislabelled mislabeled_idxs, len(mislabeled_idxs), mislabeled_idxs.float().sum() . (TensorCategory([False, True, False, True, False, True, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, True, True, False, False, False], device=&#39;cuda:0&#39;), 32, TensorCategory(8., device=&#39;cuda:0&#39;)) . Then we index into the mislabelled items and calclulate the probabilities. . We also index into the mislabelled targets. . mislabeled_probas = learn.pred[mislabeled_idxs].softmax(-1) mislabeled_targs = learn.y[mislabeled_idxs] . Then we pick the probability of the predicted class . predicted_probas = mislabeled_probas.max(-1).values predicted_probas . tensor([0.7182, 0.6797, 0.6050, 0.6181, 0.4840, 0.4588, 0.5954, 0.7079], device=&#39;cuda:0&#39;, grad_fn=&lt;MaxBackward0&gt;) . We also store the class of the mislabelled items . predicted_targs = mislabeled_probas.max(-1).indices predicted_targs . tensor([2, 1, 0, 3, 0, 0, 1, 2], device=&#39;cuda:0&#39;) . Here we pick the predicted probability of the actual target class (probability for the target class that the model predicted) . eye = torch.eye(dls.c).to(&#39;cuda&#39;) actual_probas = mislabeled_probas[eye[mislabeled_targs].bool()] actual_probas . tensor([0.1144, 0.2731, 0.2278, 0.2835, 0.3380, 0.2353, 0.3692, 0.1910], device=&#39;cuda:0&#39;, grad_fn=&lt;IndexBackward&gt;) . This is an important step we check if the abs difference between predicted_probas and actual_probas is above a hyperparameter theta . theta = 0.3 msk = torch.abs(predicted_probas - actual_probas) &gt; theta #there are 5 items that meets the condition msk . tensor([ True, True, True, True, False, False, False, True], device=&#39;cuda:0&#39;) . We now gather the new targets that was predicted by the model for the 5 items that meets the condition . new_targs = learn.dls.tfms[1][1].vocab[predicted_targs[msk]] new_targs . (#5) [&#39;rust&#39;,&#39;multiple_diseases&#39;,&#39;healthy&#39;,&#39;scab&#39;,&#39;rust&#39;] . Now that we have the new_targs we will update the labels for these indexes in the training set with new_targs. The theta used is progressively lowered. Hence, as the training progresses we would progressively correct the targets even if the difference between probability of the predicted class and predicted probability of actual target class is small. This means as the training progresses we take the prediction by the model as the actual instead of the label that came with the data. . That&#39;s the end of the blog. Please feel free to contact me at @arshyma (Twitter) or marshath@gmail.com if there is anything. Thank you :) .",
            "url": "https://moarshy.github.io/blogs/image_classification/fastai/2021/01/31/first-blog.html",
            "relUrl": "/image_classification/fastai/2021/01/31/first-blog.html",
            "date": " • Jan 31, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://moarshy.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://moarshy.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
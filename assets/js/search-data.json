{
  
    
        "post0": {
            "title": "What is DICOM?",
            "content": ". Note: Most of the codes used in this blog are either taken from fastbook or fastai docs . What is DICOM? . DICOM stands for Digital Imaging and COmmunication in Medicine | It is a software integration standard that is used in medical imaging | It is the standard that establishes rules that allows different medical imaging modalities (such as X-Rays, Ultrasound, CT, MRI) from different vendors and hospitals to exchange information between them | . There core of DICOM . DICOM File Format | This is the important for part for DL. | DICOM images uses the .dcm extension | .dcm allows patient data, image pixel values to be stored under different tags | Like mentioned above apart from images, DICOM also contains patient details (such as patient name and age) and image acqusition data (such as type of equipment used) | . DICOM Network protocol | The protocol allows for information exchange between different imaging modalities that is connected to the hospital network | Used for searching for images from the archive and to display images on the workstation | This protocol can also be used to monitor treatment, schedule procedures, report status | . Let&#39;s use fastai to read a dcm file and use it to understand the information it contains. We will use the SIIM-ACR Pneumothorax Segentation dataset. . ! pip install fastai -q --upgrade ! pip install pydicom kornia opencv-python scikit-image nbdev -q . |████████████████████████████████| 194kB 18.8MB/s |████████████████████████████████| 61kB 10.7MB/s |████████████████████████████████| 1.9MB 18.6MB/s |████████████████████████████████| 225kB 46.4MB/s |████████████████████████████████| 51kB 8.3MB/s . import fastai print(fastai.__version__) . 2.2.5 . from fastai.basics import * from fastai.callback.all import * from fastai.vision.all import * from fastai.medical.imaging import * import pydicom import pandas as pd . # downloading the dataset pneumothorax_source = untar_data(URLs.SIIM_SMALL) . # reading the dcm files items = get_dicom_files(pneumothorax_source/f&quot;train/&quot;) . # lets read the dcm file for patient 11 patient = 11 xray_sample = items[patient].dcmread() . # lets take a look at the DICOM metafile xray_sample . Dataset.file_meta - (0002, 0000) File Meta Information Group Length UL: 202 (0002, 0001) File Meta Information Version OB: b&#39; x00 x01&#39; (0002, 0002) Media Storage SOP Class UID UI: Secondary Capture Image Storage (0002, 0003) Media Storage SOP Instance UID UI: 1.2.276.0.7230010.3.1.4.8323329.10731.1517875225.339875 (0002, 0010) Transfer Syntax UID UI: JPEG Baseline (Process 1) (0002, 0012) Implementation Class UID UI: 1.2.276.0.7230010.3.0.3.6.0 (0002, 0013) Implementation Version Name SH: &#39;OFFIS_DCMTK_360&#39; - (0008, 0005) Specific Character Set CS: &#39;ISO_IR 100&#39; (0008, 0016) SOP Class UID UI: Secondary Capture Image Storage (0008, 0018) SOP Instance UID UI: 1.2.276.0.7230010.3.1.4.8323329.10731.1517875225.339875 (0008, 0020) Study Date DA: &#39;19010101&#39; (0008, 0030) Study Time TM: &#39;000000.00&#39; (0008, 0050) Accession Number SH: &#39;&#39; (0008, 0060) Modality CS: &#39;CR&#39; (0008, 0064) Conversion Type CS: &#39;WSD&#39; (0008, 0090) Referring Physician&#39;s Name PN: &#39;&#39; (0008, 103e) Series Description LO: &#39;view: PA&#39; (0010, 0010) Patient&#39;s Name PN: &#39;8d26a8e1-0913-4952-b5c9-3dd09b3c8925&#39; (0010, 0020) Patient ID LO: &#39;8d26a8e1-0913-4952-b5c9-3dd09b3c8925&#39; (0010, 0030) Patient&#39;s Birth Date DA: &#39;&#39; (0010, 0040) Patient&#39;s Sex CS: &#39;F&#39; (0010, 1010) Patient&#39;s Age AS: &#39;67&#39; (0018, 0015) Body Part Examined CS: &#39;CHEST&#39; (0018, 5101) View Position CS: &#39;PA&#39; (0020, 000d) Study Instance UID UI: 1.2.276.0.7230010.3.1.2.8323329.10731.1517875225.339874 (0020, 000e) Series Instance UID UI: 1.2.276.0.7230010.3.1.3.8323329.10731.1517875225.339873 (0020, 0010) Study ID SH: &#39;&#39; (0020, 0011) Series Number IS: &#34;1&#34; (0020, 0013) Instance Number IS: &#34;1&#34; (0020, 0020) Patient Orientation CS: &#39;&#39; (0028, 0002) Samples per Pixel US: 1 (0028, 0004) Photometric Interpretation CS: &#39;MONOCHROME2&#39; (0028, 0010) Rows US: 1024 (0028, 0011) Columns US: 1024 (0028, 0030) Pixel Spacing DS: [0.14300000000000002, 0.14300000000000002] (0028, 0100) Bits Allocated US: 8 (0028, 0101) Bits Stored US: 8 (0028, 0102) High Bit US: 7 (0028, 0103) Pixel Representation US: 0 (0028, 2110) Lossy Image Compression CS: &#39;01&#39; (0028, 2114) Lossy Image Compression Method CS: &#39;ISO_10918_1&#39; (7fe0, 0010) Pixel Data OB: Array of 163378 elements . As can be seen, the dataset file has many rows. Each row contains a data element. . An example of a data element is (0028, 0010) Rows US: 1024 . Let&#39;s break down the data element . (0028, 0010) is the tag. There are two parts - Group (0028) and Element (0010). From our example of patient 11 above, we can see that Group 0010 groups all patient details. . In our data element example, the tag is followed by Rows which describes the data element. Following the tag and its description, the next value is Value Representation (VR) which describes the data type of the data element. In our example, the VR is US which means Unsigned Short. The VR is then followed by Value Length. In our example, there are 1024 rows. . There are 1,000s of data elements in the DICOM. In this, we will focus on the 0028 group which describes different image/pixel related attributes and (7fe0, 0010) which describes and contains the pixel data. . (0028, 0002) Samples per pixel - This indicates if the image is grayscale (1) or RGB (3). In our example, we have a grayscale image. | (0028, 0004) Photometric Interpretation - describes the color space of our image. Some of the possible values are - MONOCHROME, MONOCHROME2, PALETTE COLOR, RGB. In our case it is MONOCHROME2 where low values are dark and high values are bright. It is the opposite in MONOCHROME. PALETTE COLOR contains a color image with a single sample per pixel. RGB describes red, green and blue image planes. For RGB, samples per pixel would be 3. | (0028, 0010) Rows - describes the number of rows in the image. In our example, there are 1024 rows. | (0028, 0011) Columns - describes the number of columns in the image. In our example, there are 1024 columns. | (0028, 0030) Pixel Spacing - describes the distance between centers of two neighbouring pixels. In our example [0.19431099999999998, 0.19431099999999998], the first number is the Row Spacing, the second number is the Column Spacing. | (0028, 0100) Bits Allocated - Number of bits allocated for each pixel sample. | (0028, 0101) Bits Stored - Number of bits stored for each pixel sample. A 8 bits image would have pixel value between 0-255. | (0028, 0102) High Bit - Most significant bit for pixel sample data. Each sample shall have the same high bit. | (0028, 0103) Pixel Representation - can either be unsigned(0) or signed(1). If you are like me and need a refresher on signed vs unsigned integer, here is a link | (0028, 2110) Lossy Image Compression - Specifies whether an Image has undergone lossy compression. 00 image has not been subjected to lossy compression. 01 image has been subjected to lossy compression. lossy compression or irreversible compression is the class of data encoding methods that uses inexact approximations and partial data discarding to represent the content. These techniques are used to reduce data size for storing, handling, and transmitting content. | (0028, 2114) Lossy Image Compression Method - the methods used in Lossy Image compression. ISO_10918_1 : JPEG Lossy Compression, ISO_14495_1 : JPEG-LS Near-lossless Compression, ISO_15444_1 : JPEG 2000 Irreversible Compression : ISO_13818_2 MPEG2 Compression, ISO_14496_10 : MPEG-4 AVC/H.264 Compression, ISO_23008_2 : HEVC/H.265 Lossy Compression. In our example, it is ISO_10918_1 which is JPEG Lossy Compression. | (7fe0, 0010) Pixel Data - an array of pixel data. Data type is OB. Let&#39;s take a look below. | . Apart from the above, let&#39;s also understand the below . (0008,0060) Modality - Type of equipment that originally acquired the data used to create the images in this Series.For all the different values, refer here. Some examples are CT : Computed Tomography, CR: Computed Radiography, DX : Digital Radiography, ES : Endoscopy, IVUS : Intravascular Ultrasound | . Let&#39;s take a look at a sample of PixelData . xray_sample.PixelData[:200] . b&#39; xfe xff x00 xe0 x00 x00 x00 x00 xfe xff x00 xe0&#34;~ x02 x00 xff xd8 xff xdb x00C x00 x03 x02 x02 x02 x02 x02 x03 x02 x02 x02 x03 x03 x03 x03 x04 x06 x04 x04 x04 x04 x04 x08 x06 x06 x05 x06 t x08 n n t x08 t t n x0c x0f x0c n x0b x0e x0b t t r x11 r x0e x0f x10 x10 x11 x10 n x0c x12 x13 x12 x10 x13 x0f x10 x10 x10 xff xc0 x00 x0b x08 x04 x00 x04 x00 x01 x01 x11 x00 xff xc4 x00 x1d x00 x00 x02 x03 x01 x01 x01 x01 x01 x00 x00 x00 x00 x00 x00 x00 x00 x04 x05 x00 x03 x06 x02 x07 x01 x08 t xff xc4 x00J x10 x00 x02 x01 x03 x03 x02 x04 x03 x06 x06 x01 x02 x05 x02 x00 x0f x01 x02 x11 x00 x03! x04 x121 x05A x13&#34;Qa x06q x81 x142 x91 xa1 xb1 xf0 x07#B xc1 xd1 xe1 xf1 x15R x08$3br x16C%4Sc tD x82 x92&#39; . As the raw PixelData are complex. Let&#39;s use .pixel_array to read the data in a more familiar format. . xray_sample.pixel_array . array([[ 0, 0, 0, ..., 233, 248, 153], [ 0, 0, 0, ..., 226, 241, 151], [ 0, 0, 0, ..., 208, 223, 140], ..., [ 1, 1, 1, ..., 2, 2, 0], [ 1, 1, 1, ..., 2, 2, 0], [ 1, 1, 1, ..., 2, 2, 0]], dtype=uint8) . Let&#39;s use .show to show the image . xray_sample.show() . fastai provides the following function to create dataframe from the dicom files. Apart from reading the DICOM file, it also calculate summary statistics of the image pixels (mean/min/max/std) when px_summ is set to True . dicom_dataframe = pd.DataFrame.from_dicoms(items, px_summ=True) dicom_dataframe[:5] . SpecificCharacterSet SOPClassUID SOPInstanceUID StudyDate StudyTime AccessionNumber Modality ConversionType ReferringPhysicianName SeriesDescription PatientName PatientID PatientBirthDate PatientSex PatientAge BodyPartExamined ViewPosition StudyInstanceUID SeriesInstanceUID StudyID SeriesNumber InstanceNumber PatientOrientation SamplesPerPixel PhotometricInterpretation Rows Columns PixelSpacing BitsAllocated BitsStored HighBit PixelRepresentation LossyImageCompression LossyImageCompressionMethod fname MultiPixelSpacing PixelSpacing1 img_min img_max img_mean img_std img_pct_window . 0 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.11405.1517875232.807474 | 19010101 | 000000.00 | | CR | WSD | | view: PA | (8, 1, 5, 5, 1, 5, 9, 8, -, a, b, 5, 1, -, 4, e, 5, 7, -, a, 4, d, e, -, 8, d, 0, b, 0, 3, 9, 9, 3, c, 5, 5) | 81551598-ab51-4e57-a4de-8d0b03993c55 | | F | 66 | CHEST | PA | 1.2.276.0.7230010.3.1.2.8323329.11405.1517875232.807473 | 1.2.276.0.7230010.3.1.3.8323329.11405.1517875232.807472 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.171000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000018.dcm | 1 | 0.171000 | 0 | 252 | 150.097208 | 59.213376 | 0.136433 | . 1 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.2126.1517875171.269922 | 19010101 | 000000.00 | | CR | WSD | | view: AP | (4, 4, 2, 7, 4, 9, d, 8, -, 8, 2, 7, 9, -, 4, 3, e, d, -, a, 2, a, 3, -, 8, 5, 1, 8, 3, 9, 3, e, 0, 0, e, b) | 442749d8-8279-43ed-a2a3-8518393e00eb | | M | 28 | CHEST | AP | 1.2.276.0.7230010.3.1.2.8323329.2126.1517875171.269921 | 1.2.276.0.7230010.3.1.3.8323329.2126.1517875171.269920 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.139000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000071.dcm | 1 | 0.139000 | 0 | 255 | 144.198807 | 54.626554 | 0.071772 | . 2 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.31988.1517875157.881392 | 19010101 | 000000.00 | | CR | WSD | | view: PA | (6, 3, a, 4, 0, 2, a, 4, -, 8, 2, 8, c, -, 4, c, c, 4, -, 9, 7, 6, 0, -, 4, 3, 2, 6, 0, 2, 1, 6, 2, 4, 2, 5) | 63a402a4-828c-4cc4-9760-432602162425 | | M | 65 | CHEST | PA | 1.2.276.0.7230010.3.1.2.8323329.31988.1517875157.881391 | 1.2.276.0.7230010.3.1.3.8323329.31988.1517875157.881390 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.168000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000037.dcm | 1 | 0.168000 | 0 | 255 | 176.959857 | 51.083963 | 0.036505 | . 3 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.32498.1517875160.877894 | 19010101 | 000000.00 | | CR | WSD | | view: AP | (c, f, 2, a, 4, 6, 7, f, -, e, b, 2, 0, -, 4, 5, 1, 6, -, 8, f, 3, 6, -, 3, 6, d, e, 2, d, 4, 5, 4, 5, 4, e) | cf2a467f-eb20-4516-8f36-36de2d45454e | | F | 48 | CHEST | AP | 1.2.276.0.7230010.3.1.2.8323329.32498.1517875160.877893 | 1.2.276.0.7230010.3.1.3.8323329.32498.1517875160.877892 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.168000 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000135.dcm | 1 | 0.168000 | 0 | 255 | 107.833434 | 65.194095 | 0.241501 | . 4 ISO_IR 100 | 1.2.840.10008.5.1.4.1.1.7 | 1.2.276.0.7230010.3.1.4.8323329.2633.1517875173.805125 | 19010101 | 000000.00 | | CR | WSD | | view: PA | (9, 4, 3, a, 4, e, 9, 3, -, d, 0, 5, 8, -, 4, 8, 8, 4, -, b, c, 6, 1, -, 7, b, 6, f, b, b, 2, 8, 8, c, 8, 5) | 943a4e93-d058-4884-bc61-7b6fbb288c85 | | F | 40 | CHEST | PA | 1.2.276.0.7230010.3.1.2.8323329.2633.1517875173.805124 | 1.2.276.0.7230010.3.1.3.8323329.2633.1517875173.805123 | | 1 | 1 | | 1 | MONOCHROME2 | 1024 | 1024 | 0.194311 | 8 | 8 | 7 | 0 | 01 | ISO_10918_1 | /root/.fastai/data/siim_small/train/No Pneumothorax/000103.dcm | 1 | 0.194311 | 0 | 255 | 96.317883 | 43.559422 | 0.267043 | . We have 250 samples which wouldn&#39;t be sufficient to build anything of significance. We will use it to understand the DICOM data and to learn how fastai can be used to work with medical images. . len(dicom_dataframe) . 250 . Let&#39;s take a look at the different columns. img_min, img_max, img_mean, img_std, img_pct_window are calculated by from_dicom fastai function. . dicom_dataframe.columns . Index([&#39;SpecificCharacterSet&#39;, &#39;SOPClassUID&#39;, &#39;SOPInstanceUID&#39;, &#39;StudyDate&#39;, &#39;StudyTime&#39;, &#39;AccessionNumber&#39;, &#39;Modality&#39;, &#39;ConversionType&#39;, &#39;ReferringPhysicianName&#39;, &#39;SeriesDescription&#39;, &#39;PatientName&#39;, &#39;PatientID&#39;, &#39;PatientBirthDate&#39;, &#39;PatientSex&#39;, &#39;PatientAge&#39;, &#39;BodyPartExamined&#39;, &#39;ViewPosition&#39;, &#39;StudyInstanceUID&#39;, &#39;SeriesInstanceUID&#39;, &#39;StudyID&#39;, &#39;SeriesNumber&#39;, &#39;InstanceNumber&#39;, &#39;PatientOrientation&#39;, &#39;SamplesPerPixel&#39;, &#39;PhotometricInterpretation&#39;, &#39;Rows&#39;, &#39;Columns&#39;, &#39;PixelSpacing&#39;, &#39;BitsAllocated&#39;, &#39;BitsStored&#39;, &#39;HighBit&#39;, &#39;PixelRepresentation&#39;, &#39;LossyImageCompression&#39;, &#39;LossyImageCompressionMethod&#39;, &#39;fname&#39;, &#39;MultiPixelSpacing&#39;, &#39;PixelSpacing1&#39;, &#39;img_min&#39;, &#39;img_max&#39;, &#39;img_mean&#39;, &#39;img_std&#39;, &#39;img_pct_window&#39;], dtype=&#39;object&#39;) . We have CR as the Modelity which is Computed Radiograpy . dicom_dataframe[&#39;Modality&#39;].unique() . array([&#39;CR&#39;], dtype=object) . We can see the age distribution of the patient. . plt.style.use(&#39;seaborn&#39;) dicom_dataframe[&#39;PatientAge&#39;].astype(int).hist(bins=10) plt.xlabel(&#39;Age&#39;) plt.ylabel(&#39;Frequency&#39;) . Text(0, 0.5, &#39;Frequency&#39;) . We have an equal number of M and F . dicom_dataframe[&#39;PatientSex&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa9c9cebe10&gt; . Building a classifier with fastai . Next, let&#39;s use fastai.medical to build a simple classifier. Again, we only have 250 samples which won&#39;t be enough to build anything of significance. The goal is to show how we can use fastai.medical to work with medical images. . Let&#39;s get the different folders/files. . pneumothorax_source.ls() . (#2) [Path(&#39;/root/.fastai/data/siim_small/train&#39;),Path(&#39;/root/.fastai/data/siim_small/labels.csv&#39;)] . Let&#39;s read the labels.csv and see what is in it . train = pd.read_csv(pneumothorax_source/&#39;labels.csv&#39;) . Let&#39;s build a simple dataloader . pneumothorax = DataBlock(blocks=(ImageBlock(cls=PILDicom), CategoryBlock), get_x=lambda x:pneumothorax_source/f&quot;{x[0]}&quot;, get_y=lambda x:x[1], splitter=RandomSplitter(), batch_tfms=aug_transforms(size=400)) dls = pneumothorax.dataloaders(train.values, bs=8) . Once, we have made the dataloader, we can take a look at a batch. . dls.show_batch(max_n=8) . Let&#39;s use fastai&#39;s xrestnet model - xresnet is based on &quot;Bag of Tricks for ResNet&quot; paper. We also use Mish activation instead of the usual ReLU and we will use self-attention. . model = xresnet50(pretrained=False, act_cls=Mish, sa=True, n_out=2) . model[0][0] . Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) . # Here, we will set the first layer to accept single channel image model[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) . For the optimizer, we will use ranger which uses RAdam and Lookahead. . learn = Learner(dls, model=model, loss_func=LabelSmoothingCrossEntropy(), metrics= accuracy, opt_func=ranger) . learn.lr_find() . SuggestedLRs(lr_min=3.311311302240938e-05, lr_steep=0.0012022644514217973) . learn.fit_flat_cos(5, 5e-4) . epoch train_loss valid_loss accuracy time . 0 | 0.635151 | 0.681318 | 0.680000 | 00:13 | . 1 | 0.629701 | 0.652602 | 0.680000 | 00:12 | . 2 | 0.614139 | 0.611938 | 0.720000 | 00:13 | . 3 | 0.601922 | 0.743685 | 0.640000 | 00:12 | . 4 | 0.596375 | 0.660376 | 0.700000 | 00:12 | . Interpreting the classifier with fastai . Among the many cool things fastai provides, interpretation is one. Let&#39;s take a look at the classifier and see how well our classifier is doing. . learn.show_results(max_n=8) . # Let&#39;s initiate a ClassificationInterpretation interp = ClassificationInterpretation.from_learner(learn) . Let&#39;s take a look at our top_losses. Our classifier confuses No Pneumothorax for Pneumothorax. This is likely because of the lack of training data. Again, the goal here is to understand how we can use fastai and its many tools. . interp.plot_top_losses(6, figsize=(14,8)) . As expected, there is a lot of False Negative (predicts &quot;No Pneumothorax&quot; when it is &quot;Pneumothorax&quot;) . interp.plot_confusion_matrix(figsize=(7,7)) . CAM and GradCAM . CAM - Class Activation Map . . credits: https://docs.paperspace.com/machine-learning/wiki/interpretability . Interpretability or explainability is the degree to which a model&#39;s prediction/decision can be explained in human terms. This is a huge area of research as often ML models are said to be a black-box with no interpretability. There are certain tools being developed to address this area. Among those tools are CAM and GradCAM. . Class Activation Map (CAM) uses the activation of the last convolution layer and the predictions of the last layer to plot heatmap visualization. The visualization gives an idea of why the model made its decision. In medical imaging, this sort of heatmap visualization could augment radiologists and other doctors apart from doing the classification. Fastbook has a chapter dedicated to CAM and GradCAM which can be found here. . Let&#39;s see how we can make use of CAM. . Below, we define a hook class. Hooks are similar to callbacks and they let us inject codes into forward and backward calculation. . class Hook(): def hook_func(self, m, i, o): self.stored = o.detach().clone() . Let&#39;s define the path for no_pneumothorax and pneumothorax class . nopneumo = (pneumothorax_source/&#39;train&#39;).ls()[0].ls() pneumo = (pneumothorax_source/&#39;train&#39;).ls()[1].ls() . Then, we initiate the Hook class and use the register_forward_hook to attach the hook class to the forward function. learn.model[-5] would access the whole xresnet model without the head and register_forward_hook would be able to attach our hook to the last convolution layer. . hook_output = Hook() hook = learn.model[-5].register_forward_hook(hook_output.hook_func) . Let&#39;s define a function to grab a sampel of image either from nopneumo or pneumo folders. . def grab_x(path, patient): x = first(dls.test_dl([path[patient]])) return x[0] . Let&#39;s define a function to get the CAM map. As you can see we make use of the einsum function. It is awesome funtion and here is one of my fav video on this topic. . def get_cammap(x): with torch.no_grad(): output = learn.model.eval()(x) act = hook_output.stored[0] print(F.softmax(output, dim=-1)) cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[-1].weight, act) hook.remove() return cam_map . Then, Let&#39;s define a function to plot two images - left_image = input image and right_image = input image superimposed by the CAM activation heatmap. idx=0 to see nopneumo class activation and idx=1 to see pneumo class activation. . def plot_cam(x, cls, cam_map, img_size=400): x_dec = TensorDicom(dls.train.decode((x,))[0][0]) _,ax = plt.subplots(1,2, figsize=(15,10)) x_dec.show(ctx=ax[0]) x_dec.show(ctx=ax[1]) ax[1].imshow(cam_map[cls].detach().cpu(), alpha=0.6, extent=(0,img_size,img_size,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . Let&#39;s write a function to wrap everything. . def get_plot_cam_image(path, patient, cls): x = grab_x(path, patient) cam_map = get_cammap(x) plot_cam(x, cls, cam_map, img_size=400) . Below, is an example for No pneumothorax. Areas in bright yellow/orange corresponds to high activations while areas in purple corresponds to low activations. Unfortunately, our classifier hasn&#39;t learnt much to show this. Hence, lets see pic from fastbook. . . The activation map on the cat allows one to peek into model&#39;s &#39;reasons&#39; for its prediction. In medical imaging, this might highlight tumors and other such abnormalities that the radiologists could further scrutanize. . get_plot_image(nopneumo, 5, 0) . tensor([[0.8117, 0.1883]], device=&#39;cuda:0&#39;) . Below is the same image but observing for class=1 or for Pneumothorax. Some of the bright orange are around the lungs as oppose to above where for No Pneumothorax the lungs appeared purple highlighting no activation around the lungs. . get_plot_image(nopneumo, 5, 1) . tensor([[0.8117, 0.1883]], device=&#39;cuda:0&#39;) . GradCAM . Having seen how to use CAM in fastai. Let&#39;s take a look at GradCAM. . GradCAM is similar to CAM except in GradCAM we make use of the gradient to plot the visualization. Because we use gradient, we are able to plot the visualization for the earlier conv layers too. With CAM, we were only able to observe the visualization for the final conv layer because once we obtained the activation of the conv layer, we need to multiply by the last weight matrix.This method only works for the final conv layer. This variant was introduced in the paper - &quot;Grad-CAM: Why Did You Say That? Visual Explanations from Deep Networks via Gradient-based Localization&quot; in 2016. . # A hook to store the output of a layer class Hook(): def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func) def hook_func(self, m, i, o): self.stored = o.detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() # A hook to store the grad of a layer class HookBwd(): def __init__(self, m): self.hook = m.register_backward_hook(self.hook_func) def hook_func(self, m, gi, go): self.stored = go[0].detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . def get_gradcammap(x, cls, model_layer): with HookBwd(model_layer) as hookg: with Hook(model_layer) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) return cam_map . def plot_gcam(x, img_size=400): x_dec = TensorDicom(dls.train.decode((x,))[0][0]) _,ax = plt.subplots(1,2, figsize=(15,10)) x_dec.show(ctx=ax[0]) x_dec.show(ctx=ax[1]) ax[1].imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,img_size,img_size,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . def get_plot_gcam_image(path, patient, cls, layer=-5): x = grab_x(path, patient) cam_map = get_gradcammap(x, cls, layer) plot_gcam(x) . get_plot_gcam_image(nopneumo, 7, cls=0, layer=learn.model[-5]) . get_plot_gcam_image(nopneumo, 7, cls=1, layer=learn.model[-5]) . get_plot_gcam_image(nopneumo, 7, cls=0, layer=learn.model[-5]) . References . https://docs.fast.ai/tutorial.medical_imaging.html . http://dicomiseasy.blogspot.com/2011/10/introduction-to-dicom-chapter-1.html . http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.3.html#sect_C.7.6.3.1.4 . https://dicom.innolitics.com/ciods/ct-image/image-plane/00280030#:~:text=All%20pixel%20spacing%20related%20Attributes,adjacent%20rows%2C%20or%20vertical%20spacing. .",
            "url": "https://moarshy.github.io/blogs/fastai/medical/interpretability/2021/02/07/whatisdicom.html",
            "relUrl": "/fastai/medical/interpretability/2021/02/07/whatisdicom.html",
            "date": " • Feb 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Image Classification Techniques",
            "content": "This is going to be my first blog. I would like to start by stating the motivation for starting this. The main reason for starting is because people I respect in the deep learning (DL) community have all advocated for blogging as part of the learning process. Hence, I am hoping to articulate my learning and understanding through these blogs. It also means I am open to anyone correcting my understanding as well as to add to my current understanding. My blog is mostly going to be around DL and fastai. This year one of my goals is to be around the fastai community so I could learn from the amazing people and the conversation that takes place there. . In this blog, we will go through methods/techniques that help in image classification tasks. The following are the techniques I have been learning and as much as I can I would reference where I learnt the techniques from so anyone could learn from the source. The examples/codes will be using the fastai library. . Image classification is possibly the first task one would encounter when learning DL. Image classification is a computer vision task where a model classifies an image. For example, a cat or dog classifier classifies whether an image is a cat or a dog. . The types of image classifiction tasks . binary image classification - a task in which the model has to predict between two classes (eg. cat or dog) | multi-class image classification - a classification task in which the model has to predict between n-classes (eg. cat, dog, horse or bear) | multi-label image classification - a classification task in which the model has to predict between n-classes and in each prediction there can be one or more than one predictions. (eg. cat and dog) | Throughout this blog we will make use of the Plant Pathology dataset from Kaggle to understand how the different techniques can be applied. . So first, lets understand our dataset. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . !pip uninstall fastai -q -y !pip install fastai --upgrade -q . |████████████████████████████████| 194kB 17.2MB/s |████████████████████████████████| 61kB 9.7MB/s . from fastai.vision.all import * from sklearn.model_selection import StratifiedKFold . SEED=101 set_seed(SEED) path = Path(&#39;/content/drive/MyDrive/colab_notebooks/fastai/plant_pathology/data&#39;) train = pd.read_csv(path/&#39;train.csv&#39;) train.head(3) . image_id healthy multiple_diseases rust scab . 0 Train_0 | 0 | 0 | 0 | 1 | . 1 Train_1 | 0 | 1 | 0 | 0 | . 2 Train_2 | 1 | 0 | 0 | 0 | . Let&#39;s look at the data. As can see from the above, our train.csv contains the image_id and the labels. There are four classes - healthy, multiple_diseases, rust and scab . train[&#39;labels&#39;] = train.iloc[:, 1:].idxmax(1) train[&#39;labels&#39;].value_counts(), len(train) . (rust 622 scab 592 healthy 516 multiple_diseases 91 Name: labels, dtype: int64, 1821) . In total, there are 1,821 train images. Except for the multiple_diseases class, all other classes have similar number of training examples. One of the problems with this dataset is the relatively low number of multiple_diseases examples in the dataset. Later, we will see how we can use oversampling to help with this. . Now let&#39;s start with the first technique. . 1. k-fold crossvalidation . Oftentimes, training data is scarce and you might want to use all the given data in training but because in crossvalidation (train-validation split) some percentage of data is kept for validation, and that data becomes unavailable for training our model. This is where k-fold crossvalidation could be useful. How does this work? . Create k folds of validation data | Train k models using different validation set each time | During inference, make prediction on all k models and average the results | This way not only we will be ensembling k models during inference, we would also use all the data in the training process. . Let&#39;s see how this is done. . N_FOLDS = 3 train[&#39;fold&#39;] = -1 strat_kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True) for i, (_, test_index) in enumerate(strat_kfold.split(train.image_id.values, train[&#39;labels&#39;].values)): train.iloc[test_index, -1] = i train[&#39;fold&#39;] = train[&#39;fold&#39;].astype(&#39;int&#39;) . train.head(5) . image_id healthy multiple_diseases rust scab labels fold . 0 Train_0 | 0 | 0 | 0 | 1 | scab | 2 | . 1 Train_1 | 0 | 1 | 0 | 0 | multiple_diseases | 2 | . 2 Train_2 | 1 | 0 | 0 | 0 | healthy | 0 | . 3 Train_3 | 0 | 0 | 1 | 0 | rust | 1 | . 4 Train_4 | 1 | 0 | 0 | 0 | healthy | 2 | . We have 3 folds (or 3 differenct validation sets) . train[&#39;fold&#39;].value_counts() . 2 607 1 607 0 607 Name: fold, dtype: int64 . train.groupby([&#39;fold&#39;, &#39;labels&#39;]).size() . fold labels 0 healthy 172 multiple_diseases 30 rust 207 scab 198 1 healthy 172 multiple_diseases 31 rust 207 scab 197 2 healthy 172 multiple_diseases 30 rust 208 scab 197 dtype: int64 . So we have created three validation sets with each sets having 607 samples. Also because we used stratified k-fold, the different classes in each validation sets are about the same. Now we are ready to proceed with training our k models. . 2. Oversampling and undersampling . As we saw earlier, multiple_diseases class has only 90 samples as compared to others averaging around 500+. This might disadvantage the multiple_diseases class as the model might learn to predict multiple_diseases less often to improve the metrics. . In such scenarios, oversampling can be used. Oversampling is nothing but copy-pasting the same training data of a certain class to increase its numbers. . Let&#39;s see how this is done. . def oversampling(df, fold, col2os=&#39;multiple_diseases&#39;, oversampling=3): train_df_no_val = df[df[&#39;fold&#39;] != {fold}] #training set train_df_just_val = df[df[&#39;fold&#39;] == {fold}] #validation set #we only want oversample the multiple_disease class in the training set train_df_bal = pd.concat( [train_df_no_val[train_df_no_val[&#39;labels&#39;] != col2os], train_df_just_val] + [train_df_no_val[train_df_no_val[&#39;labels&#39;] == col2os]] * oversampling ).sample(frac=1.0, random_state=SEED).reset_index(drop=True) train_df_bal.reset_index(drop=True) return train_df_bal . train_os = oversampling(train, 0) . We have more data in train_os where we have used oversampling of multiple_diseases class . len(train), len(train_os) . (1821, 2003) . train_fold0 = train[train[&#39;fold&#39;] != 0] train_os_fold0 = train_os[train_os[&#39;fold&#39;] != 0] . (print(&#39;train without oversamplig&#39;, &#39; n n&#39;, train_fold0[&#39;labels&#39;].value_counts(), &#39; n n&#39;, &#39;train with oversampling&#39;, &#39; n n&#39;, train_os_fold0[&#39;labels&#39;].value_counts(), sep=&quot;&quot;)) . train without oversamplig rust 415 scab 394 healthy 344 multiple_diseases 61 Name: labels, dtype: int64 train with oversampling rust 415 scab 394 healthy 344 multiple_diseases 183 Name: labels, dtype: int64 . As we can see we have 3x our multiple_diseases class after using oversampling. Samples of other classes stay the same. Oversampling as well as its counterpart undersampling can be useful in balancing the sample size of different classes in the dataset. This allows the model to be trained with less bias towards any of the classes. . 3. Techniques from fastbook Chapter 7 . Fastbook is an amazing resource to learn DL and it is my go to resource. In Chapter 7, advanced techniques for training an image classification model are introduced. Let&#39;s see what these techniques are. . Normalization | Data augmentation including MixUp (CutMix) | Progressive resizing | Test time augmentation | Normalization . We know that having the mean and std of our input data around 0 and 1 helps the model train more efficiently and helps in generalization. Hence, normalization is almost a default technique these days. . Generally, when we train image classification we start by transfer learning. These models would have been generally trained using the ImageNet dataset. Hence, when we normalize our data we use the mean and std of ImageNet dataset to normalize our data. . If we are training from scratch, it&#39;s recommended to calculate the mean and std of the dataset for the 3-channels and use that to normalize the data. Also, during inference, the test data should be normalized using whatever stats that were used to normalize during the training. . Doing this in fastai is very easy. Let&#39;s take a look. . Let&#39;s write a function to make our dataloader . def get_dls(fold, df, img_sz=224): datablock = DataBlock( blocks=(ImageBlock, CategoryBlock()), getters=[ ColReader(&#39;image_id&#39;, pref=path/&#39;images&#39;, suff=&#39;.jpg&#39;), ColReader(&#39;labels&#39;) ], splitter=IndexSplitter(df.loc[df.fold==fold].index), item_tfms=Resize(img_sz), ) return datablock.dataloaders(source=df, bs=32) . dls = get_dls(0, train_os) x, y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.3879, 0.5049, 0.2897], device=&#39;cuda:0&#39;), TensorImage([0.1893, 0.1820, 0.1703], device=&#39;cuda:0&#39;)) . Our mean and std are nowhere near 0 and 1. . Below is how we could calculate the mean and std of our dataset. . Note: This would only use the train dataset. Hence, a more stringent way would be to use all images and calculate the mean and std. . m,s = [0., 0., 0.], [0., 0., 0.] count = 0 for x, y in next(iter(dls)): m += np.array(x.mean(dim=[0,2,3]).cpu()) s += np.array(x.std(dim=[0,2,3]).cpu()) count += 1 . m/count , s/count . (array([0.39644337, 0.51481164, 0.30797708]), array([0.19386025, 0.17980762, 0.17882748])) . Let&#39;s modify our get_dls function slightly in the batch_tfms argument. We have normalize and are using imagenet_stats to normalize the data. Let&#39;s see what is imagenet_stats first and see how this has changed our mean and std. . def get_dls(fold, df, img_sz): datablock = DataBlock( blocks=(ImageBlock, CategoryBlock()), getters=[ ColReader(&#39;image_id&#39;, pref=path/&#39;images&#39;, suff=&#39;.jpg&#39;), ColReader(&#39;labels&#39;) ], splitter=IndexSplitter(df.loc[df.fold==fold].index), item_tfms=Resize(img_sz), batch_tfms=[Normalize.from_stats(*imagenet_stats)] ) return datablock.dataloaders(source=df, bs=32) . As can be seen below, imagenet_stats is a tuple that has the mean and std for the three channels. If you have the stats for your dataset, you could also pass it similarly to normalize the data. . imagenet_stats . ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) . dls = get_dls(0, train_os, 224) x, y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([-0.3794, 0.2188, -0.4551], device=&#39;cuda:0&#39;), TensorImage([0.9271, 0.8399, 0.8564], device=&#39;cuda:0&#39;)) . The mean and std following normalization is much closer to 0 and 1. . Data augmentation, MixUp and CutMix . Data augmentation is a well known technique to improve image classification. Fastai provides many of these data augmentation tools and they can be easily applied while creating the dataloaders like how we normalized earlier. Data augmentation can be passed as an argument in either item_tfms or batch_tfms while creating our datablock. The difference between the both is that the former make use of CPU while the latter make use of GPU. Hence, batch_tfms is the preferred method to carry out most of the augmentation. . Data augmentation essentially allows us to enlarge our dataset size without getting new data. Data augmentation essentially uses synthetic data manipulation to create new images/training data. . Let&#39;s use this image to see some examples of the many data augmentation that comes with fastai. . img = PILImage(PILImage.create((path/&#39;images&#39;).ls()[SEED]).resize((600,400))) img . FlipItem flips the image at the given probability p . _,axs = subplots(2, 4) for ax in axs.flatten(): show_image(FlipItem(p=0.5)(img, split_idx=0), ctx=ax) . Another technique is dihedral, let&#39;s see what it does . _,axs = subplots(2, 4) for ax in axs.flatten(): show_image(DihedralItem(p=1.)(img, split_idx=0), ctx=ax) . RandomCropping the image at given size . _,axs = subplots(2, 4) for ax in axs.flatten(): show_image(RandomCrop(224)(img, split_idx=0), ctx=ax) . And aug_transforms which is an &quot;Utility func to easily create a list of flip, rotate, zoom, warp, lighting transforms.&quot; . timg = TensorImage(array(img)).permute(2,0,1).float()/255. def _batch_ex(bs): return TensorImage(timg[None].expand(bs, *timg.shape).clone()) . Each image is different although the input image was the same . tfms = aug_transforms(pad_mode=&#39;zeros&#39;, mult=2, min_scale=0.5) y = _batch_ex(9) for t in tfms: y = t(y, split_idx=0) _,axs = plt.subplots(2,3, figsize=(12,10)) for i,ax in enumerate(axs.flatten()): show_image(y[i], ctx=ax) . MixUp . MixUp is a data augmentation technique that was introduced in 2018 in this paper. So what happens during a MixUp? . for a training image image_1, a second image image_2 is randomly selected | new_image is created following this formula where alpha is a constant between 0. and 1.0 that is used to mix the two images | new_image = alpha * image_1 + (1-alpha) * image_2 . similarly, the targets of image_1 and image_2 are blended to create new_target | For example, let&#39;s assume we are training a 4-class model and the &gt;one-hot-encode for image_1 is [0., 0., 1., 0.] and image_2 is [0., 0., &gt;0., 1.]. Also, let&#39;s assume alpha is 0.3. The target for our new_image is &gt;[0., 0., 0.3, 0.7]. . new_target = 0.3 * [0., 0., 1., 0.] + (1-0.3) * [0., 0., 0., 1.] . With this, now, we have completely new image for training. | Let&#39;s use the codes from fastai docs to see how our MixUp images look . mixup = MixUp(1.) with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=mixup) as learn: learn.epoch,learn.training = 0,True learn.dl = dls.train b = dls.one_batch() learn._split(b) learn(&#39;before_batch&#39;) _,axs = plt.subplots(3,3, figsize=(9,9)) dls.show_batch(b=(mixup.x,mixup.y), ctxs=axs.flatten()) . epoch train_loss valid_loss time . 0 | 00:01 | . As can be seen, some of our images above look a bit smeared/odd that is because of mixup. As can be seen, using MixUp with fastai is relatively easy. It is passed as a callback argument when we initiate a Learner. It can also be passed as a cbs in fit_one_cycle. . learn.fit_one_cycle(3, cbs=MixUp(1.0)) . Note: the alpha we passed when we initiate the mixup will be used to generate a distribution the size of batch_size hence the alpha varies from one image to another. Below an example of generating the alpha distribution . torch.distributions.beta.Beta(tensor(1.), tensor(1.)).sample((10,)) . tensor([0.4794, 0.3758, 0.1914, 0.6586, 0.6198, 0.5889, 0.1123, 0.9081, 0.2395, 0.4103]) . CutMix . Although CutMix was not covered in the book, it has been added to the fastai library. CutMix is similar to MixUp but instead of blending images together, CutMix works by cropping a portion of image_2 and placing it in image_1. CutMix has been shown to work better than MixUp. . . Source: CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features . Let&#39;s see some examples of CutMix in action . cutmix = CutMix(1.) with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=cutmix) as learn: learn.epoch,learn.training = 0,True learn.dl = dls.train b = dls.one_batch() learn._split(b) learn(&#39;before_batch&#39;) _,axs = plt.subplots(3,3, figsize=(9,9)) dls.show_batch(b=(cutmix.x,cutmix.y), ctxs=axs.flatten()) . epoch train_loss valid_loss time . 0 | 00:01 | . Progressive resizing . As stated in the book, progressive resizing gradually uses larger and larger images as we continue our training. This technique is akin to transfer learning. Our model learns on smaller images and as we increase the image size it carries forward what it had learnt in previous training as well as picks up something additional from the larger images. . Test time augmentation . As taken from the fastbook, &quot;test time augmentation (TTA): During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.&quot; . Other things . Different architectures . Generally varying sizes of ResNet would be the first model to try and establish a baseline. After which one could explore other architecture such as efficientnet or the recently released Visual Transformer. . Transfer Learning . In most cases, transfer learning works really well hence it could be the first thing to try for most classification tasks. . 4. Techniques I learned from Zach&#39;s walkwithfastai imagewoof lecture . I highly recommend walkwithfastai course. It is also my go to resource for fastai. In this particular notebook, Zach introduces different techniques that seem to work really well for image classification tasks. Please check the notebook for references and details. Zach also has a lecture using this notebook here. . The notebook introduces the following techniques. . xresnet which is an arch based on the &quot;Bag of Tricks for ResNet&quot; paper | Mish - a new activation function | ranger - a new optimizer that combines RAdam and Lookahead | Self-attention | MaxBlurPool | a different LR scheduler - that uses flatten+anneal scheduling | Label Smoothing Cross Entropy | 5. Softlabeling and progressive label correction . Softlabeling . I came across softlabeling through Isaac Flath&#39;s amazing blog. I think the blog is the best place to get started on softlabeling. . In supervised learning, labels, which are created by humans, could be erroneous. This leads to the labels being &#39;noisy&#39;. This was the case in the Plant Pathology competition and the winner used a similar method (softlabeling) in the winning solution. . How do we deal with such noisy labels? One way is to punish the model less for predicting incorrectly a noisy label. The steps are as follows . Create a k-fold crossvalidation | Train k classifier using different k-fold for sufficient epochs using the noisy labels | Use the classifier to predict on the kth validation set and save the prediction | Upon completion of the above step, you will have two labels - one the noisy label that came with the data label_ori and another predicted by the above classifiers label_pred | Finally, train your actual classifier and this time when labels between label_ori and label_pred differs, adjust the labels using a hyperparameter a | For example, let&#39;s assume we are training a 4-class model and the one-hot-encode for label_ori is [0., 0., 1., 0.] and label_pred is [0., 0., 0., 1.]. Also, let&#39;s assume a is 0.5. Our new_label would be [0., 0., 0.5, 0.5]. By doing this, the model would be punished less for predicting the wrong class as this could be due to noisy labels. . a = 0.5 label_ori = [0., 0., 1., 0.] label_pred = [0., 0., 0., 1.] new_label = a * label_ori + (1-a) * label_pred new_label = [0., 0., 0.5, 0.5] . Progressive Label Correction . I came across this technique in thsi wonderful Kaggle Notebook by Kerem Turgutlu. It is a paper implementation of this paper. . Again, this method works in cases where there are noisy labels in the dataset. This is my understanding of how it is implemented. . During model training, we let a model train normally for a warm_up period. In the above implementation, the warm_up period was 20% of the total iterations. | Once training goes over the warm_up iterations, Progressive Label Correction (PLC) kicks-in | In PLC, after an iteration, mislabeled indexes are identified | Then, we calculate the probabilities of the max prediction class (predicted_probas - the class the model predicted) as well as the actual target class (actual_probas - the class the model should have predicted) of the mislabeled indexes | we check if the absolute difference between predicted_probas and the actual_probas is above a theta value (theta is a hyperparameter we set) | if the difference is higher, then for those mislabeled indexes, we change (‘correct’) the label y to be that predicted by the model | we continue step 2 to step 6 while progressively lowering the theta using a scheduler function (linear scheduler was used in the above notebook). | Let&#39;s take a look what the ProgressiveLabelCorrection callback in the notebook does. . dls = get_dls(0, train_os, 128) . learn = cnn_learner(dls, resnet18, pretrained=True) . Lets assume our learner has been trained for warm_up iterations and see how PLC is applied using this one_batch . learn.fit_one_cycle(1) learn.one_batch(5, learn.dls.one_batch()) . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss time . 0 | 1.573150 | 1.102266 | 02:34 | . . 28.57% [6/21 01:46&lt;04:27 1.5732] Here after an iteration, we check the predicted class and compare it to the target (y) to get the mislabelled indexes . preds_max = learn.pred.argmax(-1) mislabeled_idxs = preds_max != learn.y #so we have 32 samples in each iteration (which is the batch_size), of which 8 are mislabelled mislabeled_idxs, len(mislabeled_idxs), mislabeled_idxs.float().sum() . (TensorCategory([False, True, False, True, False, True, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, True, True, False, False, False], device=&#39;cuda:0&#39;), 32, TensorCategory(8., device=&#39;cuda:0&#39;)) . Then we index into the mislabelled items and calclulate the probabilities. . We also index into the mislabelled targets. . mislabeled_probas = learn.pred[mislabeled_idxs].softmax(-1) mislabeled_targs = learn.y[mislabeled_idxs] . Then we pick the probability of the predicted class . predicted_probas = mislabeled_probas.max(-1).values predicted_probas . tensor([0.7182, 0.6797, 0.6050, 0.6181, 0.4840, 0.4588, 0.5954, 0.7079], device=&#39;cuda:0&#39;, grad_fn=&lt;MaxBackward0&gt;) . We also store the class of the mislabelled items . predicted_targs = mislabeled_probas.max(-1).indices predicted_targs . tensor([2, 1, 0, 3, 0, 0, 1, 2], device=&#39;cuda:0&#39;) . Here we pick the predicted probability of the actual target class (probability for the target class that the model predicted) . eye = torch.eye(dls.c).to(&#39;cuda&#39;) actual_probas = mislabeled_probas[eye[mislabeled_targs].bool()] actual_probas . tensor([0.1144, 0.2731, 0.2278, 0.2835, 0.3380, 0.2353, 0.3692, 0.1910], device=&#39;cuda:0&#39;, grad_fn=&lt;IndexBackward&gt;) . This is an important step we check if the abs difference between predicted_probas and actual_probas is above a hyperparameter theta . theta = 0.3 msk = torch.abs(predicted_probas - actual_probas) &gt; theta #there are 5 items that meets the condition msk . tensor([ True, True, True, True, False, False, False, True], device=&#39;cuda:0&#39;) . We now gather the new targets that was predicted by the model for the 5 items that meets the condition . new_targs = learn.dls.tfms[1][1].vocab[predicted_targs[msk]] new_targs . (#5) [&#39;rust&#39;,&#39;multiple_diseases&#39;,&#39;healthy&#39;,&#39;scab&#39;,&#39;rust&#39;] . Now that we have the new_targs we will update the labels for these indexes in the training set with new_targs. The theta used is progressively lowered. Hence, as the training progresses we would progressively correct the targets even if the difference between probability of the predicted class and predicted probability of actual target class is small. This means as the training progresses we take the prediction by the model as the actual instead of the label that came with the data. . That&#39;s the end of the blog. Please feel free to contact me at @arshyma (Twitter) or marshath@gmail.com if there is anything. Thank you :) .",
            "url": "https://moarshy.github.io/blogs/image_classification/fastai/2021/01/31/first-blog.html",
            "relUrl": "/image_classification/fastai/2021/01/31/first-blog.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://moarshy.github.io/blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://moarshy.github.io/blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://moarshy.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://moarshy.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}